[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Taylor_series/index (5).html",
    "href": "posts/Taylor_series/index (5).html",
    "title": "Taylor series",
    "section": "",
    "text": "The Taylor series is a mathematical representation of a function as an infinite sum of terms. It is mainly used to approximate a non-polynomial function in terms of polynomials of degree n. Higher the degree n, the better the approximation.\nThe general form of the Taylor series for the function \\(f(x)\\) centered about the point a is given by:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} +...\n\\]\nLet’s say we take the function \\(y = sin(x)\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-63, 64)]\ny = [math.sin(xi) for xi in x]\n\n\nplt.plot(x, y,)\nplt.title('Plot of y = sin(x)')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.show()\n\n\n\n\nLet’s choose a=0 and let’s go up to n=5, then using the Taylor series we can write,\n\\[\nsin(x) = 0 + cos(0)(x)+\\frac{-sin(0)x^2}{2!}+\\frac{-cos(0)x^3}{3!}+\\frac{sin(0)x^4}{4!}+\\frac{cos(0)x^5}{5!}\n\\]\nwhere \\(f(x)=sin(x),f'(x)=cos(x),f''(x)=-sin(x),f'''(x)=-cos(x),f''''(x)=sin(x),and f'''''(x)=cos(x)\\)\nTherefore,\n\\(sin(x) = x-\\frac{x^3}{3!}+\\frac{x^5}{5!}\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-33, 34)]\ny = [xi-((xi**3)/6)+((xi**5)/120) for xi in x]\ny1 = [math.sin(xi) for xi in x]\nplt.figure()\nplt.plot(x, y,label=\"y=x-(x^3)/3!+(x^5)/5!\")\nplt.plot(x,y1,label=\"y=sinx\")\nplt.title('Approximating sin(x) using y = x - (x^3)/3! + (x^5)/5!')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nWe can see above that, the polynomial function is providing us a good approximation of the sine function but only for values close to zero (as we selected a to be 0). For other values the approximation is inaccurate.\nWhen we truncate the infinite Taylor series to a polynomial of degree n, we call it the Taylor polynomial of degree n. The higher the value of n, the more accurate our approximation is."
  },
  {
    "objectID": "posts/Taylor_series/index (5).html#what-is-the-taylor-series",
    "href": "posts/Taylor_series/index (5).html#what-is-the-taylor-series",
    "title": "Taylor series",
    "section": "",
    "text": "The Taylor series is a mathematical representation of a function as an infinite sum of terms. It is mainly used to approximate a non-polynomial function in terms of polynomials of degree n. Higher the degree n, the better the approximation.\nThe general form of the Taylor series for the function \\(f(x)\\) centered about the point a is given by:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} +...\n\\]\nLet’s say we take the function \\(y = sin(x)\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-63, 64)]\ny = [math.sin(xi) for xi in x]\n\n\nplt.plot(x, y,)\nplt.title('Plot of y = sin(x)')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.show()\n\n\n\n\nLet’s choose a=0 and let’s go up to n=5, then using the Taylor series we can write,\n\\[\nsin(x) = 0 + cos(0)(x)+\\frac{-sin(0)x^2}{2!}+\\frac{-cos(0)x^3}{3!}+\\frac{sin(0)x^4}{4!}+\\frac{cos(0)x^5}{5!}\n\\]\nwhere \\(f(x)=sin(x),f'(x)=cos(x),f''(x)=-sin(x),f'''(x)=-cos(x),f''''(x)=sin(x),and f'''''(x)=cos(x)\\)\nTherefore,\n\\(sin(x) = x-\\frac{x^3}{3!}+\\frac{x^5}{5!}\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-33, 34)]\ny = [xi-((xi**3)/6)+((xi**5)/120) for xi in x]\ny1 = [math.sin(xi) for xi in x]\nplt.figure()\nplt.plot(x, y,label=\"y=x-(x^3)/3!+(x^5)/5!\")\nplt.plot(x,y1,label=\"y=sinx\")\nplt.title('Approximating sin(x) using y = x - (x^3)/3! + (x^5)/5!')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nWe can see above that, the polynomial function is providing us a good approximation of the sine function but only for values close to zero (as we selected a to be 0). For other values the approximation is inaccurate.\nWhen we truncate the infinite Taylor series to a polynomial of degree n, we call it the Taylor polynomial of degree n. The higher the value of n, the more accurate our approximation is."
  },
  {
    "objectID": "posts/Taylor_series/index (5).html#taylor-series-for-bivariate-functions",
    "href": "posts/Taylor_series/index (5).html#taylor-series-for-bivariate-functions",
    "title": "Taylor series",
    "section": "Taylor series for bivariate functions",
    "text": "Taylor series for bivariate functions\nWe can generalize the Taylor series for functions depending on multiple variables.\nLet’s discuss the Taylor series for bivariate functions. The formula is given by,\n\\[\nf(x,y)= f(a,b)+\\frac{\\partial f}{\\partial x}(x-a)+\\frac{\\partial f}{\\partial y}(y-b)+\\frac{\\partial^2 f}{\\partial x^2}\\frac{(x-a)^2}{2!}+\\frac{\\partial^2 f}{\\partial y^2}\\frac{(y-b)^2}{2!}+...\n\\]\nHere the approximation of the function takes place near the point,\\((a,b)\\), and the partial derivatives are calculated at the point \\((a,b)\\).\nLet’s take an example.\n\\[\nf(x,y)=x^2+y^2\n\\]\nWe can try to approximate this function using the 2D Taylor series around the point \\((1,1)\\) and let’s choose the value of n as 2.\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx1 = np.linspace(-5,5,100)\ny1 = np.linspace(-5,5,100)\nX1,Y1 =np.meshgrid(x1,y1)\nZ_true=X1**2 + Y1**2\n\nfig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.plot_surface(X1,Y1,Z_true,cmap=\"plasma\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nax.view_init(elev=20, azim=30)\nplt.show()\n\n\n\n\nLet us try to approximate this function using the Taylor series with a=b=1 and n =2.\n\\[\n\\begin{equation} \\begin{split}\nf(x,y)&=2+2(x-1)+2(y-1)+(x-1)^2+(y-1)^2\\\\\nf(x,y)&=x^2+y^2 \\end{split} \\end{equation}\n\\]\nwhich is the true value indeed. Hence, the Taylor series exactly approximates \\(f(x,y)\\) when centered about the point \\((1,1)\\).\nYou can visualize the approximations of the Taylor series in the link given below.\nhttps://vannshtaylor.streamlit.app/"
  },
  {
    "objectID": "posts/Eigenvalue/index (6).html",
    "href": "posts/Eigenvalue/index (6).html",
    "title": "Matrix Decompositions",
    "section": "",
    "text": "1. Eigenvalue Decomposition\nMatrix decompositions are needed to solve large system of linear equations or diagonalize a matrix on a computer. Eigenvalue decomposition of a square matrix is factorizing it into a product of three matrices representing it’s eigenvectors and eigenvalues.\n\nLet’s say we take a NxN shaped matrix A and we want to decompose it.\nwe consider a matrix U such that all the eigenvectors of A are the column vectors of U.\nThen we can say\n\\[   U=\\begin{bmatrix} u_1 & u_2 \\cdots&u_n  \\end{bmatrix}  \\]\nwhere \\(u_1,u_2\\cdots u_n\\) are eigenvectors of A.\nIf we post-multiply A with U we get,\n\\[   \\begin{equation} \\begin{split} AU &= \\begin{bmatrix} Au_1 & Au_2 \\cdots&Au_n  \\end{bmatrix} \\\\ &= \\begin{bmatrix} \\lambda_1 u_1 & \\lambda_2u_2 \\cdots&\\lambda_nu_n  \\end{bmatrix} \\end{split} \\end{equation}  \\]\nas \\(u_i\\) is the eigenvector of A \\(\\forall \\quad i \\in[1,..n]\\quad Au_i=\\lambda_iu_i\\), where \\(\\lambda_i\\) is the corresponding eigenvalue.\nNext, we can split the RHS into U and a diagonal matrix containing the eigenvalues of A\\((\\Lambda)\\). \\[\\begin{equation} \\begin{split} AU &=\\begin{bmatrix} u_1 & u_2 \\cdots&u_n  \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 \\cdots& 0 \\\\ 0 & \\lambda_2 \\cdots& 0 \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ 0 & 0 \\cdots& \\lambda_n \\end{bmatrix} \\\\ AU &= U\\Lambda \\end{split} \\end{equation}\\]\nNow, we know that U is an invertible matrix(column vectors of U are linearly independant as they are the eigenvectors of A) hence, we can post-multiply with \\(U^{-1}\\) on both sides.\n\\[  A=U\\Lambda U^{-1}  \\]\nThe eigenvalue decomposition of A is hence complete.\nThere is a special case to the above scenario. If A is a symmetric matrix, then it’s eigenvalues are real and it’s eigenvectors are orthogonal\\((i.e. v_i.v_j=0 \\quad where \\quad i \\not=j)\\). Further, if the eigenvectors are also orthonormal\\((i.e. v_i.v_i=1 \\quad where \\quad i \\in[1,..n])\\), then we can conclude that,\n\\[   Q=U^TU=I  \\]and hence to calculate the inverse of a symmetric matrix, we can calculate it’s transpose which reduces the complexity by n-fold.\nMoreover, we can generalize to higher powers of A,\n\\[  \\begin{equation} \\begin{split} A &= U\\Lambda U^{-1} \\\\ A^2 &= U\\Lambda^2 U^{-1} \\\\ \\vdots \\\\ A^n &= U\\Lambda^n U^{-1} \\end{split} \\end{equation}  \\]\nIf we have to compute higher powers of A, we can use the right hand side of the above equation to compute the higher power of \\(\\Lambda\\) which is a diagonal matrix. The higher power of a diagonal matrix can be calculated by raising the diagonal elements to that power, and thus, eigenvalue deconposition can reduce the computational cost.\nLet’s now apply eigenvalue decomposition to a matrix."
  },
  {
    "objectID": "posts/shannonfano/index 2 (2).html",
    "href": "posts/shannonfano/index 2 (2).html",
    "title": "Shannon Fano encoding",
    "section": "",
    "text": "Shannon Fano encoding\nThe Shannon fano algorithm is a lossless data compression technique. Data compression is the process of encoding or converting data in such a way that it consumes less memory space. Hence, it reduces the resources required to store and transmit data.\nFor eg, it would make more sense to assign symbols or characters occuring more frequently to consume less number of bits than to assign equal bits to all characters. Hence, it assigns codes of variable lengths to each character. In the realm of data compression, two distinct but connected methods for creating a prefix code for the characters is Shannon Fano encoding.\n\n\nAlgorithm\n\nWe find out the frequency or probability of occurance of each character or symbol and store it in an array.\nWe sort the array in decreasing order based on probability(frequency).\nThen, we split the list in two parts with as much similarity(least variance) between the frequency/probability of left and right parts.\nWe assign the binary digit ‘0’ to the left part and binary digit ‘1’ to the right part. This implies that symbols in the left section will begin with ‘0’ and symbols in the right section will begin with ‘1’.\nWe repeat steps 3 and 4 by splitting each section into left and right sections and assigning the binary digits ‘0’ and ‘1’ to them unitl each section is left with just one symbol(analogous to leaf node of binary tree).\n\nLet’s better understand shannon-fano and visualize it using examples. We’ll encode the characters of the following sentence.\n“In the tranquil stillness of the early morning, birdsong filled the air as the first rays of sunlight painted the horizon with hues of pink and orange, creating a breathtakingly beautiful scene that seemed to belong to a world untouched by time.”\nLet’s first calculate the probability of each character in the above statement, and sort them based on probabilities in descending order.\n\nsentence = \"aaaaaaaabbbbccd\"\ndef generate_prob(sentence):\n    d={}\n    special=0\n    for c in sentence:\n        if c==\",\" or c==\".\" or c==\" \":\n            special+=1\n            continue\n        elif c.lower() in d:\n            d[c.lower()] += 1\n        else:\n            d[c.lower()] = 1\n    total_char = len(sentence)-special\n    for key in d:\n        d[key] = d[key]/total_char\n    return d\nseq = generate_prob(sentence)\nseq=sorted(seq.items(),key=lambda item: item[1], reverse=True)\nd = dict(seq)\nd\n\n{'a': 0.5333333333333333,\n 'b': 0.26666666666666666,\n 'c': 0.13333333333333333,\n 'd': 0.06666666666666667}\n\n\nWe create nodes for the sequence and build a tree following the top to bottom approach.\nSince we want the splits to have similar total probabilities, we can assign ‘0’ and ‘1’ alternatively on the descended probability sequence or we can create the left subset until the probabilities of all the subsets approximately reach half of the total probability value.\n\nclass Node:\n    def __init__(self,char,prob):\n        self.char=char\n        self.p=prob\n        self.code=\"\"\n        self.left=None\n        self.right=None\n\ninitial=\"\"\nfor c in seq:\n    initial+=c[0]\nroot = Node(initial,1)\n\n\ndef split(node):\n    left_subset={}\n    right_subset={}\n    total_p = node.p\n    half_p = total_p/2\n    sum=0\n    index = 0\n    seq=node.char\n    for c in seq:\n        if sum&gt;=half_p:\n            break\n        sum+=d[c]\n        index+=1\n    left_char = seq[:index]\n    right_char = seq[index:]\n    for l in left_char:\n        left_subset[l]=d[l]\n    for r in right_char:\n        right_subset[r]=d[r]\n    return left_subset,right_subset\n\ndef Build_tree(root):\n    if len(root.char)==1:\n        return\n    left_subset,right_subset = split(root)\n    left_char=\"\"\n    right_char=\"\"\n\n    for c in left_subset:\n        left_char+=c\n    for cr in right_subset:\n        right_char+=cr\n    left_prob = sum(left_subset.values())\n    right_prob = sum(right_subset.values())\n\n    left_node = Node(char=left_char,prob=left_prob)\n    right_node = Node(char=right_char,prob=right_prob)\n    root.left = left_node\n    root.right = right_node\n    Build_tree(left_node)\n    Build_tree(right_node)\n    \nBuild_tree(root)\n\ndef assign_codes(root):\n    if root.left is None and root.right is None:\n        return\n    if root.left is not None:\n        root.left.code = \"0\" + root.code\n    if root.right is not None:\n        root.right.code = \"1\" + root.code\n    assign_codes(root.left)\n    assign_codes(root.right)\nassign_codes(root)\n\ntotal_bits=0\ndef print_codes(root):\n    global total_bits\n    if root is not None:\n        if (len(root.char)==1):\n          total_bits += len(root.code)*sentence.count(root.char)\n          print(f\"Character is '{root.char}', probability is {round(root.p,4)} and code is {root.code}\")\n        print_codes(root.left)\n        print_codes(root.right)\nprint_codes(root)\n\nCharacter is 'a', probability is 0.5333 and code is 0\nCharacter is 'b', probability is 0.2667 and code is 01\nCharacter is 'c', probability is 0.1333 and code is 011\nCharacter is 'd', probability is 0.0667 and code is 111\n\n\nHence, we follow the top-bottom approach, where we add ‘0’ to the left subset and ‘1’ to the right subset as we traverse down the tree.\n\n\nVisualizing the Binary Tree\n\n\n\n\n\n\n\nResults and Observations\nIf we consider the “ascii” encoding, we would have to consider 8 bits for each letter, whereas we consume less number of bits using shannon-fano encoding.\n\nprint(f\"No.of bits using ascii = {len(sentence)*8}\")\nprint(f\"No.of bits using shannon-fano = {total_bits}\")\n\nNo.of bits using ascii = 120\nNo.of bits using shannon-fano = 25\n\n\nShannon-fano algorithm certainly performs better than the ascii encoding and does a decent job.\n\nCompression ratio\nThe compression ratio tells us how much smaller the compressed data is compared to the original data. The higher the compression ratio, the better the compression.\nCompression ratio = Before Compression / After compression\n\nb = len(sentence)*8\na = total_bits\nprint(f\"compression ratio is,\",(b/a))\n\ncompression ratio is, 4.8\n\n\n\n\nEntropy\nEntropy is the measure of randomness of data, higher the entropy more random the data. More random the data, the more number of bits we need to measure it.\n\\[\nH(x) = -\\Sigma_{i=1}^n p(x_i)log_2(p(x_i))\n\\]\n\nimport numpy as np\ndef calc_entropy(p):\n  return -1 * p * np.log2(p)\n\nentropy = 0\nfor c in d:\n  entropy += calc_entropy(d[c])\n  \nprint(f\"Entropy of input data is {entropy}\")\n\nEntropy of input data is 1.640223928941852\n\n\nIt turns out, shannon fano algorithm is not the most optimized algorithm and may not work always. The Huffman encoding which is a slightly modified version of the shannon-fano optimizes the data compression even further.\nhttps://vannshsf.streamlit.app/"
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html",
    "href": "posts/2ndorder/index(2) (1).html",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "Hessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by H.\n\\[\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n\\]\nFor example, let’s take a bivariate function(n=2),\n\\[\nf(x,y) = xy^2\n\\]\n\\[\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n\\]\nHere, \\(\\frac{\\partial f}{\\partial x}=y^2\\), \\(\\frac{\\partial f}{\\partial y}=2xy\\), \\(\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y\\)\n\\[\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n\\]\nUsing this matrix, we can find out the nature of the curvature at any point \\((x_1,y_1)\\), by substituting this point in the Hessian.\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n\n\n\n\n\n\nFollowing are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5718e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses,X,Y,Z):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Initial point\")\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Final point\")\n  plt.legend()\n  plt.show()\n  \nplot_contour(guesses,X,Y,Z)\n\n\n\n\n\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess. Depending upon different initial guesses, the final guess could land onto different minimasor possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses,X,Y,Z)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n/var/folders/76/s25grzw958ld2_m4fhl5j5900000gn/T/ipykernel_49912/298132734.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n  Z = f(torch.tensor([X, Y])).detach().numpy()\n\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima.\n\n\n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "href": "posts/2ndorder/index(2) (1).html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "Following are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5718e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses,X,Y,Z):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Initial point\")\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Final point\")\n  plt.legend()\n  plt.show()\n  \nplot_contour(guesses,X,Y,Z)\n\n\n\n\n\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess. Depending upon different initial guesses, the final guess could land onto different minimasor possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses,X,Y,Z)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#l-bfgs-for-optimizing-functions",
    "href": "posts/2ndorder/index(2) (1).html#l-bfgs-for-optimizing-functions",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "The BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n/var/folders/76/s25grzw958ld2_m4fhl5j5900000gn/T/ipykernel_49912/298132734.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n  Z = f(torch.tensor([X, Y])).detach().numpy()\n\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#remarks1",
    "href": "posts/2ndorder/index(2) (1).html#remarks1",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "The LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#footnotes",
    "href": "posts/2ndorder/index(2) (1).html#footnotes",
    "title": "Second order Optimization methods.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023).↩︎"
  },
  {
    "objectID": "posts/sirentask/siren.html",
    "href": "posts/sirentask/siren.html",
    "title": "Siren",
    "section": "",
    "text": "Implicit Neural representations using sinusoidal activation\nThe reason we use the sine function as an activation function is because it is infinitely differentiable where as for a function like relu, the double derivative becomes zero. Let’s try regenerating an audio file by mapping coordinates in the grid with amplitudes\nThe following is the ground truth audio\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe are using a neural network with 3 hidden layers and each layer consisting of 256 neurons and we are using mean squared error loss function and optimizing it using Adam with a learning rate of 0.0001. We are training the neural network for a total of 1000 steps and displaying every 100th result. As can be seen in the following figures as the number of steps increases both the graphs converge.\nDescription of neural network\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=1, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.025424\nStep 100, Total loss 0.002972\nStep 200, Total loss 0.001123\nStep 300, Total loss 0.000861\nStep 400, Total loss 0.000702\nStep 500, Total loss 0.000586\nStep 600, Total loss 0.000535\nStep 700, Total loss 0.000538\nStep 800, Total loss 0.000665\nStep 900, Total loss 0.000362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph the coordinates of the grid are squeezed to a single dimension (x-axis) and the corresponding model output is shown on the y-axis.\nFollowing is the audio regenerated using a neural network with sinusoidal activation\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThere is a little bit of noise in the above regenerated audio signal but overall has a good performance\nLet’s try using a different activation function such as relu and how well it can perform to regenerate audio and compare it with sinusoidal activation\n\n\nStep 0, Total loss 0.032153\nStep 100, Total loss 0.032153\nStep 200, Total loss 0.032153\nStep 300, Total loss 0.032153\nStep 400, Total loss 0.032153\nStep 500, Total loss 0.032153\nStep 600, Total loss 0.032153\nStep 700, Total loss 0.032153\nStep 800, Total loss 0.032153\nStep 900, Total loss 0.032153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing is the audio regenerated through relu activation which is not at all close to the ground truth\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHow much memory do we save through this compression technique?\n\n\nMemory of audio file: 1232886 bytes\n\n\n\n\nNumber of parameters in the neural network are 198145\n\n\n\n\nMemory consumed by the neural network is 796343 bytes"
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html",
    "href": "posts/markov chain/index 2 (1).html",
    "title": "Markov Chains",
    "section": "",
    "text": "Markov models are mathematical models which are used to model sequential data, where the current observation is dependant on the past observations. Markov chains are the simplest markov models wherein, the current observation is only dependant on the previous observation and not dependant on observations prior to previous observations.\nThis can be represented by,\n\\[\nP(x_t|x_1,x_2,...x_{t-1})=P(x_t|x_{t-1})\n\\]\nFollowing is how we can represent the dependencies in data\n\n\n\n\n\nWe can can calculate probability of a sequence as follows,\n\\[\nP(x_1,x_2,..x_n)=P(x_1)P(x_2|x_1)P(x_3|x_2)...P(x_n|x_{n-1})\n\\]\nWhat this basically says is, The probability of any observation \\(x_i\\), is only dependant on \\(x_{i-1} \\quad \\forall \\quad i \\in [2,n]\\).\n\\(P(x_1)\\), is called the prior probability for the state/observation initially. The prior probability is the probability for starting from one of the states. It is denoted by \\(\\pi_i=P(x_1=i)\\), where \\(i\\) denotes the initial state from all possible states. The prior probability is one of the parameters of the Markov chain model.\nAnother parameter for the Markov chain model is the transition matrix denoted by \\(A\\). If there are \\(K\\) states, the transition matrix will be a \\(K\\)x\\(K\\) matrix, where \\(A_{ij}=P(x_t=j|x_{t-1}=i)\\).\nLet’s take an example for 3 states.\nWe can use the markov chain model to predict which city we will go to next, given the city we are in currently.\nLet’s assume the 3 cities/states to be Bangalore, Chennai and Mumbai and following is the transition matrix for this example.\n\n\n\n\nBangalore\nChennai\nMumbai\n\n\n\n\nBangalore\n0.3\n0.4\n0.3\n\n\nChennai\n0.2\n0.1\n0.7\n\n\nMumbai\n0.4\n0.4\n0.2\n\n\n\nThe sum of each row of the transition matrix must sum up to 1 as it covers all the possibilities.\nFollowing is the markov transition graph, which shows the probabilities of going from one city to another.\n\n\n\n\n\n\n\nWe can generate a sequence of observations using the following,\n\nSelect initial state\\((x_1)\\) using \\(\\pi\\).\nSample the state\\((x_t)\\) from \\(A\\) and \\(x_{t-1}\\), for \\(t \\in[2,..,T]\\).\n\nLet’s take the above example and generate a sequence of 6 time stamps.\n\n\n\n\n\n\n\n\n\n\n\n\nprior probability\n\n\n\n\nBangalore\n0.4\n\n\nChennai\n0.2\n\n\nMumbai\n0.4\n\n\n\n\n\n\n\nYou can change the values of prior probabilities and probabilities in the transition matrix in the link given below.\nhttps://vannshmarkovchain.streamlit.app/\n\n\n\nLet’s understand some properties of markov chains with the following example\n\n\n\n\n\nWe know that if there is an arrow from state A to state B, then there is a non-zero transition probability from state A to state B. If we start random walk from state “0” in the above diagram, we can never come back to state zero even after infinite steps. Such a state where we cannot come back to is called a transient state. Hence, in the above example “0” is a transient state. If we look at state “1” or state “2” we know that we are bound to come back to the same state after some steps. Such a state is called a recurrent state. Here, state “1” and “2” are examples of recurrent states.\nIn a markov chain, if all states not reachable from all other states, we say that the markov chain is reducible. In the above example, we cannot reach state “0” from state “1” or “2”.\nIf we add an arrow from state “2” to state “0”, then it is possible to come back to state “0” and hence, “0” is not a transient state any more.\n\n\n\n\n\nSuch a markov chain, where it is possible to go from every state to another(not necessarily in one move) is called an ergodic markov chain.\n\n\n\nLet’s consider the cities example again, suppose we wanted to calculate the probability of travelling from Bangalore to Chennai in 2 steps, we will have to consider all possible cases.\n\\[\nA=\\begin{bmatrix} 0.3 & 0.4 & 0.3 \\\\ 0.2 & 0.1 & 0.7 \\\\ 0.4 & 0.4 & 0.2 \\end{bmatrix}\n\\]\nLet’s denote probability of going from state i to state j in n steps as \\(p_{ij}(n)\\). In this example 0 denotes Bangalore, 1 denotes Chennai and 2 denotes Mumbai.\n\\[\np_{01}(2) = p_{02}(1)p_{21}(1)+p_{00}(1)p_{01}(1)+p_{01}(1)p_{11}(1)\n\\]\nNote that \\(p_{02}(1)\\) is just equal to \\(A_{02}\\). Hence, the above equation can be written as\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= A_{02}A_{21}+A_{00}A_{01}+A_{01}A_{11} \\\\ p_{01}(2) &= \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\end{bmatrix} \\begin{bmatrix} A_{01} \\\\ A_{11} \\\\ A_{21} \\end{bmatrix} \\end{split} \\end{equation}\n\\]\nHence \\(p_{01}(2)\\) is the dot product of the 0th row and 1st column of matrix A. Hence \\(p_{01}(2)\\) is the element present in the 0th row and the 1st column in \\(AXA\\) or \\(A^2\\). Let’s verify it.\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= 0.3*0.4+0.3*0.4+0.4*0.1 \\\\ p_{01}(2) &= 0.28 \\\\ A^2 &= \\begin{bmatrix} 0.29 & 0.28 & 0.43 \\\\ 0.36 & 0.37 & 0.27\\\\ 0.28 & 0.28 & 0.44 \\end{bmatrix}\\end{split}\\end{equation}\n\\]\nHence, \\(A^n_{ij}\\) denotes the probability of moving from state i to state j in n steps. If some power of the trasnition matrix A has all positive values then it is possible to move from every state to every other where the number of steps is equal to the power taken of A and hence, A is ergodic."
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#markov-chain-sampling",
    "href": "posts/markov chain/index 2 (1).html#markov-chain-sampling",
    "title": "Markov Chains",
    "section": "",
    "text": "We can generate a sequence of observations using the following,\n\nSelect initial state\\((x_1)\\) using \\(\\pi\\).\nSample the state\\((x_t)\\) from \\(A\\) and \\(x_{t-1}\\), for \\(t \\in[2,..,T]\\).\n\nLet’s take the above example and generate a sequence of 6 time stamps.\n\n\n\n\n\n\n\n\n\n\n\n\nprior probability\n\n\n\n\nBangalore\n0.4\n\n\nChennai\n0.2\n\n\nMumbai\n0.4\n\n\n\n\n\n\n\nYou can change the values of prior probabilities and probabilities in the transition matrix in the link given below.\nhttps://vannshmarkovchain.streamlit.app/"
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#markov-chain-properties",
    "href": "posts/markov chain/index 2 (1).html#markov-chain-properties",
    "title": "Markov Chains",
    "section": "",
    "text": "Let’s understand some properties of markov chains with the following example\n\n\n\n\n\nWe know that if there is an arrow from state A to state B, then there is a non-zero transition probability from state A to state B. If we start random walk from state “0” in the above diagram, we can never come back to state zero even after infinite steps. Such a state where we cannot come back to is called a transient state. Hence, in the above example “0” is a transient state. If we look at state “1” or state “2” we know that we are bound to come back to the same state after some steps. Such a state is called a recurrent state. Here, state “1” and “2” are examples of recurrent states.\nIn a markov chain, if all states not reachable from all other states, we say that the markov chain is reducible. In the above example, we cannot reach state “0” from state “1” or “2”.\nIf we add an arrow from state “2” to state “0”, then it is possible to come back to state “0” and hence, “0” is not a transient state any more.\n\n\n\n\n\nSuch a markov chain, where it is possible to go from every state to another(not necessarily in one move) is called an ergodic markov chain."
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#transition-in-n-steps",
    "href": "posts/markov chain/index 2 (1).html#transition-in-n-steps",
    "title": "Markov Chains",
    "section": "",
    "text": "Let’s consider the cities example again, suppose we wanted to calculate the probability of travelling from Bangalore to Chennai in 2 steps, we will have to consider all possible cases.\n\\[\nA=\\begin{bmatrix} 0.3 & 0.4 & 0.3 \\\\ 0.2 & 0.1 & 0.7 \\\\ 0.4 & 0.4 & 0.2 \\end{bmatrix}\n\\]\nLet’s denote probability of going from state i to state j in n steps as \\(p_{ij}(n)\\). In this example 0 denotes Bangalore, 1 denotes Chennai and 2 denotes Mumbai.\n\\[\np_{01}(2) = p_{02}(1)p_{21}(1)+p_{00}(1)p_{01}(1)+p_{01}(1)p_{11}(1)\n\\]\nNote that \\(p_{02}(1)\\) is just equal to \\(A_{02}\\). Hence, the above equation can be written as\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= A_{02}A_{21}+A_{00}A_{01}+A_{01}A_{11} \\\\ p_{01}(2) &= \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\end{bmatrix} \\begin{bmatrix} A_{01} \\\\ A_{11} \\\\ A_{21} \\end{bmatrix} \\end{split} \\end{equation}\n\\]\nHence \\(p_{01}(2)\\) is the dot product of the 0th row and 1st column of matrix A. Hence \\(p_{01}(2)\\) is the element present in the 0th row and the 1st column in \\(AXA\\) or \\(A^2\\). Let’s verify it.\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= 0.3*0.4+0.3*0.4+0.4*0.1 \\\\ p_{01}(2) &= 0.28 \\\\ A^2 &= \\begin{bmatrix} 0.29 & 0.28 & 0.43 \\\\ 0.36 & 0.37 & 0.27\\\\ 0.28 & 0.28 & 0.44 \\end{bmatrix}\\end{split}\\end{equation}\n\\]\nHence, \\(A^n_{ij}\\) denotes the probability of moving from state i to state j in n steps. If some power of the trasnition matrix A has all positive values then it is possible to move from every state to every other where the number of steps is equal to the power taken of A and hence, A is ergodic."
  },
  {
    "objectID": "posts/matrix low rank/index (3).html",
    "href": "posts/matrix low rank/index (3).html",
    "title": "Matrix transformations and Low-rank matrices",
    "section": "",
    "text": "Matrix Transformations\nMatrix transformation or linear transformation of a vector v is pre-multiplying the vector by a matrix (called the transformation matrix) which results in v being transformed.\nLet us first interpret matrix transformations in the 2D space.\n\nfrom sympy import Matrix, MatrixSymbol, Eq, MatMul\n\nsympy_v = MatrixSymbol(\"v\",2,1) # Column vector having 2 rows\nsympy_A = MatrixSymbol(\"A\",2,2) # 2x2 Transformation matrix\nsympy_y = MatrixSymbol(\"y\", 2, 1)\nprint(\"y is the transformed vector.\")\nEq(sympy_y,sympy_A*sympy_v,evaluate=False)\n\ny is the transformed vector.\n\n\n\\(\\displaystyle y = A v\\)\n\n\n\nExample\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[3,3],[2,6]])\nv = np.array([1,1])\nAv = A @ v\nplt.arrow(0,0,1,1,head_width=0.2,width=0.05,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.2,width=0.05,color='red',label=\"Av\")\nplt.legend([f\"v = {v}\",f\"Av = {Av}\"],loc='lower right')\nplt.title(f\"A = {A}\")\nplt.show()\n\n\n\n\n\nEq(Matrix(Av),MatMul(Matrix(A),Matrix(v)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}6\\\\8\\end{matrix}\\right] = \\left[\\begin{matrix}3 & 3\\\\2 & 6\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\nIn the above equation,\nA = \\(\\begin{bmatrix} 3&3\\\\2&6\\end{bmatrix}\\), v = \\(\\begin{bmatrix} 1\\\\1 \\end{bmatrix}\\) and y = \\(\\begin{bmatrix} 6\\\\8 \\end{bmatrix}\\)\n\n\n\n\nNote:\n\n\nIf the transformation matrix A is taken to be \\(\\begin{bmatrix} \\cos(\\theta)&-\\sin(\\theta)\\\\ \\sin(\\theta)&\\cos(\\theta) \\end{bmatrix}\\)\n\n\nthen the vector v is rotated in an anti-clockwise direction by an angle \\(\\theta\\).\nFor example, Let’s take v = \\(\\begin{bmatrix} 1\\\\0 \\end{bmatrix}\\) and \\(\\theta\\)=90 deg, therefore A = \\(\\begin{bmatrix} 0&-1\\\\1&0 \\end{bmatrix}\\)\n\nA = np.array([[0,-1],[1,0]])\nv = np.array([1,0])\nAv = A @ v\nplt.figure()\nplt.arrow(0,0,1,0,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.legend([f\"v = {v}\",f\"Av = {Av}\"])\nplt.title(f\"A = {A}\")\nplt.show()\n\n\n\n\n\nEq(Matrix(Av),MatMul(Matrix(A),Matrix(v)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\0\\end{matrix}\\right]\\)\n\n\nAs a result of the transformation, vector v is rotated by 90 degrees in the counter-clockwise direction.\n\n\n\n\nUnderstanding Matrix transformations of low-rank matrices\nLow-rank matrices are matrices which have rank less than the dimensionality of the column space of the matrix. For eg. a 3x3 transformation matrix will be a low rank matrix if it’s column space can be spanned by less than or equal to 2 vectors. In other words, the column vectors are linearly dependant.\nExample of a low-rank matrix is \\(\\begin{bmatrix} 1&0&1\\\\0.5&1&1.5\\\\1&0&1 \\end{bmatrix}\\)\nThe determinant of the transformation matrix having low-rank is zero.\nGeometrically what this means is transforming the vector using a low-rank matrix results in the subspace of the original vector space. Hence for example if the original vector space is \\(R^3\\), the resultant subspace can be \\(R^2\\), \\(R\\), or in an extreme case even a point.\n\nExample\n\nA = np.array([[1,-1],[3,-3]]) # Low-rank 2x2 matrix\n# Example 1\nv = np.array([2,3])\nAv = A @ v\nplt.figure()\nplt.subplot(1,2,1)\nplt.arrow(0,0,2,3,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.title(f\"A = {A}\")\nx = np.linspace(-3,3)\ny = x*3\nplt.plot(x,y,color=\"black\",alpha=0.3)\nplt.legend([f\"v = {v}\",f\"Av = {Av}\",\"y=3x\"])\n\n# Example 2\nv1 = np.array([-1.7,2.2])\nplt.subplot(1,2,2)\nplt.arrow(0,0,-1.7,2.2,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.title(f\"A = {A}\")\nx = np.linspace(-3,3)\ny = x*3\nplt.plot(x,y,color=\"black\",alpha=0.3)\nplt.legend([f\"v = {v}\",f\"Av = {Av}\",\"y=3x\"])\nplt.show()\n\n\n\n\nIn the above plots, Av (transformed vector) lies on the line \\(y=3x\\), and does not depend on the coordinates of vector v. As a result, using a low-rank matrix to transform a vector results only in the subspace of the original vector space. Here the subspace is the line \\(y=3x\\) and the original vector space is \\(R^2\\)."
  },
  {
    "objectID": "posts/Log-likelihood/index 2.html",
    "href": "posts/Log-likelihood/index 2.html",
    "title": "Log-likelihood of normal distribution",
    "section": "",
    "text": "Generating samples from a univariate normal distribution of mean and variance equal to 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmean = 1\nvariance = 1\nnum_samples = 1000\nnp.random.seed(1)\nsamples = np.random.normal(loc=mean, scale=np.sqrt(variance), size=num_samples)\nprint(samples.shape)\n\n(1000,)\n\n\nLet’s analyze the samples\n\n# First 5 elements\nprint(samples[:5])\n\n[ 2.62434536  0.38824359  0.47182825 -0.07296862  1.86540763]\n\n\nVisualizing the random samples\n\nplt.hist(samples,bins=30)\nplt.title(\"Histogram of samples with mean and variance 1\")\nplt.show()\n\n\n\n\nThe likelihood for the normal distribution is given by,\n\\[\nL(\\mu,\\sigma^2;x_1,x_2,....,x_n) = \\prod_{j=1}^n f(x_j;\\mu,\\sigma^2)\n\\]\nwhere f, is the probability density function which in this case is the univariate normal distribution given by,\n\\[\nf(x,\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^\\frac{-(x-\\mu)^2}{2\\sigma^2}\n\\]\nThe likelihood of a distribution with given mean and variance is the product of value of PDF at all sample points with given mean and variance. Likelihood gives us a fair estimate or probablity as to how well the distribution is fitting the data for given parameters (mean and variance in case of normal distributon). So the probability that the normal distribution with given mean and variance will fit the samples is a product of the value of the pdf at each sample point as all these events are independant events. Hence, plotting the likelihood or the log-likelihood as function of parameters (in this case mean and variance) can give us an estimate as to which values of parameters are needed to best fit the data.\nLikelihoods are generally products of many numbers and products are not numerically stable. Hence, log-likelihood is calculated to make computations less expensive and simpler.\nTaking log on both sides,\n\\[\n\\begin{equation} \\begin{split}\nl(\\mu,\\sigma^2;x_1,x_2,...,x_n) & = ln(L(\\mu,\\sigma^2;x_1,x_2,...,x_n)) \\\\\n& = ln((2\\pi\\sigma^2)^\\frac{-n}{2} e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2})\\\\ & = ln(2\\pi\\sigma^2)^\\frac{-n}{2}+ln(e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2}))\\\\ &= \\frac{-n}{2}ln(2\\pi\\sigma^2)-\\Sigma_{j=1}^n \\frac{(x_j-\\mu)^2}{2\\sigma^2}  \\end{split} \\end{equation}\n\\]\nThe above equation can be used to compute the log-likelihood.\n\nlog_likelihoods = []\nfor mean in np.linspace(0, 2, 100):\n    for var in np.linspace(0.1, 2, 100):\n        log_likelihood = np.sum(-0.5 * np.log(2 * np.pi * var) - ((samples - mean) ** 2) / (2 * var))\n        log_likelihoods.append((mean, var, log_likelihood))\n\nlog_likelihoods = np.array(log_likelihoods)\nprint(log_likelihoods.shape)\n\n(10000, 3)\n\n\nThe log_likelihoods array is a 2D-array with mean values in the first column, variance values in the second column and likelihood values in the third column. Let’s look at the first 5 rows of the array.\n\nlog_likelihoods[:5]\n\narray([[ 0.00000000e+00,  1.00000000e-01, -9.97514834e+03],\n       [ 0.00000000e+00,  1.19191919e-01, -8.41934984e+03],\n       [ 0.00000000e+00,  1.38383838e-01, -7.30630086e+03],\n       [ 0.00000000e+00,  1.57575758e-01, -6.47285210e+03],\n       [ 0.00000000e+00,  1.76767677e-01, -5.82700895e+03]])\n\n\nA 3D plot of how log-likelihood varies with mean and variance can be seen below.\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.plot3D(log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2],color='green')\nax.set_title(\"Log-likelihood of normal distribution as a function of mean and variance\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nax.set_zlabel(\"Log-likelihood\")\nplt.show()\n\n\n\n\nWe can also visualize 3D plots in 2D using a contour plot. To do so, we can create a 100x100 grid (because we have 100 mean and variance values) and reshape Z’s shape to 100x100 so we have a value of log-likelihood for each point on the grid.\n\n# Contour plot\nfig,ax = plt.subplots()\nX,Y,Z = log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2]\nX,Y = np.meshgrid(np.unique(X),np.unique(Y))\nZ = Z.reshape(X.shape)\nplt.contourf(X,Y,Z)\nplt.colorbar()\nax.set_title(\"Contour plot\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nplt.show()\n\n\n\n\nThe values of mean and variance which will best fit the data will be those having the highest value of log-likelihood (Z-axis) and hence in the “yellow” region of the contour plot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Computational graphs and automatic differentiation\n\n\n\n\n\nUnderstanding automatic differentiation through computational graphs\n\n\n\n\n\n\nSep 26, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nConstraint Optimization\n\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nSiren\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nVannsh Jani and Viraj Vekaria\n\n\n\n\n\n\n  \n\n\n\n\nShannon Fano encoding\n\n\n\n\n\nWhat is shannon fano encoding and how is it used in data compression.\n\n\n\n\n\n\nAug 30, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMarkov Chains\n\n\n\n\n\nUnderstanding Markov chains.\n\n\n\n\n\n\nAug 27, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Decompositions\n\n\n\n\n\nUnderstanding Eigenvalue decompositions and SVD.\n\n\n\n\n\n\nAug 23, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nSecond order Optimization methods.\n\n\n\n\n\nUnderstanding optimization of functions using Newton’s method and L-BFGS.\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTaylor series\n\n\n\n\n\nUnderstanding and visualizing 1D and 2D taylor series.\n\n\n\n\n\n\nAug 14, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nLog-likelihood of normal distribution\n\n\n\n\n\nPlotting and understanding log-likelihood as a function of parameters for normal distribution.\n\n\n\n\n\n\nAug 1, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMatrix transformations and Low-rank matrices\n\n\n\n\n\nUnderstanding matrix transformations as pre-multiplying a vector with a matrix and interpreting matrix transformations using low-rank matrices.\n\n\n\n\n\n\nJul 31, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sirentask/final_notebook.html",
    "href": "posts/sirentask/final_notebook.html",
    "title": "Siren",
    "section": "",
    "text": "Implicit Neural representations using sinusoidal activation\n\n\nAudio compression\nThe reason we use the sine function as an activation function is because it is infinitely differentiable where as for a function like relu, the double derivative becomes zero. Let’s try regenerating an audio file by mapping coordinates in the grid with amplitudes\nThe following is the ground truth audio\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe are using a neural network with 3 hidden layers and each layer consisting of 256 neurons and we are using mean squared error loss function and optimizing it using Adam with a learning rate of 0.0001. We are training the neural network for a total of 1000 steps and displaying every 100th result. As can be seen in the following figures as the number of steps increases both the graphs converge.\nDescription of neural network\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=1, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.025424\nStep 100, Total loss 0.002972\nStep 200, Total loss 0.001123\nStep 300, Total loss 0.000861\nStep 400, Total loss 0.000702\nStep 500, Total loss 0.000586\nStep 600, Total loss 0.000535\nStep 700, Total loss 0.000538\nStep 800, Total loss 0.000665\nStep 900, Total loss 0.000362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph the coordinates of the grid are squeezed to a single dimension (x-axis) and the corresponding model output is shown on the y-axis.\nFollowing is the audio regenerated using a neural network with sinusoidal activation\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThere is a little bit of noise in the above regenerated audio signal but overall has a good performance\nLet’s try using a different activation function such as relu and how well it can perform to regenerate audio and compare it with sinusoidal activation\n\n\nStep 0, Total loss 0.032153\nStep 100, Total loss 0.032153\nStep 200, Total loss 0.032153\nStep 300, Total loss 0.032153\nStep 400, Total loss 0.032153\nStep 500, Total loss 0.032153\nStep 600, Total loss 0.032153\nStep 700, Total loss 0.032153\nStep 800, Total loss 0.032153\nStep 900, Total loss 0.032153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing is the audio regenerated through relu activation which is not at all close to the ground truth\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHow much memory do we save through this compression technique?\n\n\nMemory of audio file: 1232886 bytes\n\n\n\n\nNumber of parameters in the neural network are 198145\n\n\n\n\nMemory consumed by the neural network is 796343 bytes\n\n\nHence, we can conclude that passing the network parameters instead of the audio file itself can save space.\n\n\nImage compression\nFor the sake of an example let’s take the classic cameraman photo\nThis is the architecture used for modelling the image, it contains 3 hidden layers and 256 neurons in each layer using the adam optimizer with learning rate 1e-4.\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\nNow let us train SIREN model on our dataset i.e the image. The loss and the current state of the Network is displayed for every 50th element.\n\n\nStep 0, Total loss 0.325293\nStep 50, Total loss 0.012300\nStep 100, Total loss 0.008443\nStep 150, Total loss 0.006012\nStep 200, Total loss 0.004077\nStep 250, Total loss 0.002673\nStep 300, Total loss 0.001977\nStep 350, Total loss 0.001566\nStep 400, Total loss 0.001294\nStep 450, Total loss 0.001094\nStep 500, Total loss 0.000939\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s see how ReLU works in this setting. This is the architecture for the Neural Network:\n\n\nNetwork(\n  (net): Sequential(\n    (0): ReLULayer(\n      (linear): Linear(in_features=2, out_features=256, bias=True)\n    )\n    (1): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): ReLULayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.733969\nStep 50, Total loss 0.077224\nStep 100, Total loss 0.064925\nStep 150, Total loss 0.057928\nStep 200, Total loss 0.052542\nStep 250, Total loss 0.048083\nStep 300, Total loss 0.044299\nStep 350, Total loss 0.041403\nStep 400, Total loss 0.039117\nStep 450, Total loss 0.037259\nStep 500, Total loss 0.035712\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the laplacian of the image generated by ReLU activation is constant (or you can say zero) because the second derivative of the ReLU function is zero, because it is a linear function. Hence, it does not perform well for transmitting signals.\nAnother takeaway for us was that the task of these kinds of Neural nets is to actually overfit over the data. By overfitting the data, it basically memorised the outputs for the given inputs. In generic Machine learning applications sinusoidal activation functions would not work at all.\nThis method is approximately 12 times better than the regular way of transmitting images. But the catch is that training the neural network for a single image of decent quality takes an hour on fairly strong computers. This renders this message of compression very unpractical. Hence, it is not used for real world data compression yet. If somehow the time required for training could be lowered, it would be quite usable in real life applications."
  },
  {
    "objectID": "posts/constraints.html",
    "href": "posts/constraints.html",
    "title": "Constraint Optimization",
    "section": "",
    "text": "In many real life scenarios when we try to optimize a function, there will be some constraints which we need to keep in mind. constraints can be of two types, equality constraints and inequality constraints.\n\n\nWhen we want to let’ say optimize a function \\(f(\\text x)\\), where x could be a single value or a vector of n-dimensions, but we are subject to m equality constraint as follows\n\\(h_i(\\text x)=0 \\forall i \\in [1,..m]\\)\nLet’s better understand this with an example\nWe want to optimize the function \\(f(\\text x , \\text y)=x^2+y^2\\), given the equality constraint \\(h(\\text x , \\text y)=x+y-1\\). So basically we need values of x and y such that they optimize f while also satisfy h.\nFirst we need to figure out if this optimization problem is an maximizing problem or an minimizing problem. For that let’s plot f=1 and h.\n\n\n\n\n\n\n\n\n\n\nWe can see that if we keep increasing the value of f, we will always get intersection points with the line. Hence, f can go upto \\(\\infty\\). On the other hand if we decrease the value of f, we can only do it upto an extent or else f will not intersect with h and will not satisfy the constraint. Hence, this is an minimization problem. Clearly the solution to this problem will occur when we keep decreasing f until it becomes tangent to h.\nIf we consider the gradient of both functions, the gradient of the objective function is given by the red arrow which is the direction in which the function will increase the most with small change in inputs and the gradient of the constraint function is given by the blue arrow. Hence, we can conclude that when these 2 functions are tangent to each other which is the optimal case, \\(\\nabla f(\\text x^*,\\text y^*) = \\lambda \\nabla h(\\text x^*,\\text y^*)\\), their gradients at the optimal point are parallel. Here \\(\\text x^*,\\text y^*\\) are the optimal points.\n\n\nIn this method we construct a new function called the Lagrangian denoted by L.\n\\(\\text L(\\text x,\\text y,\\lambda)=f(\\text x,\\text y)+ \\lambda h(\\text x,\\text y)\\), \\(\\lambda\\) is called the lagrangian multiplier.\nWe then set \\(\\nabla L = 0\\) i.e \\(\\frac {\\partial \\text L}{\\partial \\text x}=0, \\quad \\frac {\\partial \\text L}{\\partial \\text y}=0, \\quad \\frac {\\partial \\text L}{\\partial \\lambda}=0\\)\nWe then solve for x, y and \\(\\lambda\\) using the three equations we get. Notice that \\(\\frac {\\partial \\text L}{\\partial \\lambda}=h(\\text x,\\text y)\\).Solving above, the three equations we get are\n\\[\\begin{equation}\\begin{split} 2\\text x&=\\lambda \\\\ 2\\text y&=\\lambda \\\\ \\text x + \\text y -\\text 1&=0 \\end{split}\\end{equation} \\]\nWe can see that since the equations are symmetric in x and y. Hence x must be equal to y = 0.5 and hence optimal solution is (0.5,0.5). Let’s visualize this.\n\n\n\n\n\n\n\n\n\nOur goal is to minimize f(x) such that,\n\\(\\begin{equation}\\begin{split}\\text h_i(\\text x)&=0,\\hspace{0.1cm} \\forall i=1,...m. \\\\ \\text g_i(\\text x)&&lt;=0,\\hspace{0.1cm} \\forall i=1,...n. \\end{split}\\end{equation}\\)\nHere as constraints we have m equalities and n inequalities. To solve this problem let’s first create the Lagragian function L as follows\n\\(\\text L(\\text x,\\lambda_1,...,\\lambda_m,\\mu_1,...,\\mu_n)\\hspace{0.1cm}=\\hspace{0.1cm}\\text f(\\text x)+\\sum_{i=1}^{m} \\lambda_i \\text h_i(\\text x)+\\sum_{j=1}^{n} \\mu_j \\text g_j(\\text x)\\)\nwhere, \\(\\lambda_1-\\lambda_m\\) are multipliers for the m equalities and \\(\\mu_1-\\mu_n\\) are multipliers for the n inequalities.\nWe then equate \\(\\nabla_\\text x \\text L(\\text x,\\lambda,\\mu)=0.\\) This condition is called the stationary condition which minimizes the objective function. We also equate \\(\\nabla_\\lambda \\text L(\\text x,\\lambda,\\mu)=0.\\) which is the equality condition.\nLet’s try to understand the inequality constraint with an example.\nLet f(x,y) = \\(\\text x^2 + \\text y^2\\) and let g(x) = \\(5-\\text x-\\text y\\) and g(x)\\(&lt;=0\\). Let’s consider f(x,y)=1.\n\n\n\n\n\nClearly we need to increase the value of f(x,y) such that it is atleast tangent to g(x)=0 to satisfy the constraint.\n\n\n\n\n\nNotice that in this optimum situation \\(\\mu g(\\text x,\\text y)=0\\) and we know that at point of tangency \\(g(\\text x,\\text y)=0\\). This conditon \\(\\mu g(\\text x,\\text y)=0\\) is called the complementary slackness condition and is always valid for all inequality constraints. If the optimal solution of the constraintless optimization already satisfies the constrainsts then in such as case \\(\\mu\\) for that constraint g is zero as applying that constraint doesn’t affect the solution. Note that even in this case \\(\\mu g(\\text x,\\text y)=0\\) is satisfied. Another constraint in the complementary slackness condition is that \\(\\mu_i &gt;=0\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n.\\) for the n inequalities.\nConsidering only 1 inequality constraint in an optimization problem. We know that \\(\\nabla \\text f=-\\mu \\nabla g\\). Hence, the value of \\(\\mu\\) is \\(\\frac {-\\nabla \\text f}{\\nabla \\text g}\\). As we can see in the plot above the gradients of f and g are in opposite directions and so the value of \\(\\mu= \\frac {-\\nabla \\text f}{\\nabla \\text g}\\) is non-negative.\nFor the above example, \\(\\nabla \\text f =\\hspace{0.1cm}-\\mu\\nabla \\text g , \\begin{bmatrix} \\text 2x \\\\ \\text 2y \\end{bmatrix}= -\\mu \\begin{bmatrix} \\text -1 \\\\ \\text -1 \\end{bmatrix}\\)\nHence,\n\\[\\begin{equation}\\begin{split} &\\text 2x=\\mu \\\\ &\\text 2y=\\mu \\\\ &5-x-y=0 \\end{split}\\end{equation}\\]\nBy symmetry of x and y we can say that x = y = 2.5 and the value of \\(\\mu\\) is 5 which is greater than zero.\nHence, we can summarize KKT conditions as follows\n\\(\\begin{equation}\\begin{split} &1.\\nabla_\\text x \\text L(\\text x,\\lambda,\\mu)=0. \\\\ &2.\\nabla_\\lambda \\text L(\\text x,\\lambda,\\mu)=0. \\\\ &3.\\mu_i &gt;=0\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n. \\\\ &4.\\mu_i g_i(\\text x,\\text y)=0.\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n. \\end{split}\\end{equation}\\)\nKKT conditions are used in topics like ridge regression and support vector machines where there are constraints during optimization.\n\n\n\nSuppose we have an optimization problem, where we need to minimize \\(f(\\text x)\\), given inequality constraints \\(g_i \\le 0 \\hspace{0.1cm} \\forall i \\in [1,..,m]\\). The main idea is to include the constraints in the optimization function instead of the body of the optimization model. Hence, if we denote our objective function as \\(J(\\text x)\\),\n\\(J(\\text x)=f(\\text x)+\\sum_{i=1}^m P(g_i)\\), Here P is called a penalty function which is defined as \\(P(\\text x)= \\begin{cases} 0, \\text {if} \\hspace{0.1cm} \\text x \\le 0 \\\\ \\infty, \\text {if} \\hspace{0.1cm} \\text x \\gt 0 \\end{cases}\\)\nSo basically what the penalty function does is, if it’s input is a non-positive value (inequality constraint satisfied) then it output’s zero which is required as the penalty function musn’t influence the optimal value of x to minimize f. But if the constraint is not satisfied or the input to the penalty function is psotive then it results in infinity which increases the cost/objective function \\(J\\). Solving \\(J\\) is not feasible and difficult. Also, J is not a continuous function and hence not differentiable. To solve this problem we introduce lagrange variables/multipliers \\(\\mu_i \\hspace{0.1cm} \\forall i \\in [1,..m]\\). We define the Lagrange function \\(L\\) as follows which will help us approximate \\(J\\).\n\\(L(\\text x, \\mu)=f(\\text x)+\\sum_{i=1}^m \\mu_ig_i,\\hspace{0.1cm} \\mu_i \\ge 0\\). We have seen that if \\(\\mu_i=0\\) then \\(g_i\\) doesn’t affect the optimal value x.\nHow is \\(L \\hspace{0.1cm}\\text{and}\\hspace{0.1cm} J \\hspace{0.1cm}\\text {related?}\\)\n\\(J(\\text x)= \\max_{\\mu}L(\\text x,\\mu)\\) as largest possible value of \\(\\mu\\) is \\(\\infty\\).\nSo to solve the optimization problem we will now minimize \\(J(\\text x)\\) in terms of x. If \\(\\text x^*\\) is our optimal solution, then\n\\(\\text x^* = min_{\\text x}J(\\text x)\\hspace{0.1cm}=\\hspace{0.1cm}min_{\\text x}max_{\\mu}L(\\text x,\\mu)-\\text{primal problem}\\).\nLet’s try to solve this optimization problem using a different approach, let’s consider a function \\(L^*\\) such that\n\\(\\begin{equation}\\begin{split} &L^*(\\mu)=\\hspace{0.1cm}min_{\\text x}L(\\text x,\\mu) \\\\ &\\text x* = \\hspace{0.1cm} max_{\\mu}L^*(\\mu) \\\\ &\\text x*=\\hspace{0.1cm}max_{\\mu}min_{\\text x}L(\\text x,\\mu) -\\text {dual problem} \\end{split}\\end{equation}\\).\nHere we can use the concept of duality which is basically finding two different approches to solve the same problem. If we consider minimizing \\(f(\\text x)\\) in terms of x with constraints \\(g\\) to be our primal problem the maximizing the function \\(L^*(\\mu)\\) in terms of \\(\\mu\\) will be our dual problem.\nWe know from the minimax inequality that if we have a function let’s say \\(q(\\text x,\\text y)\\) then\n\\(min_{\\text x}max_{\\text y} q(\\text x,\\text y) \\ge max_{\\text y}min_{\\text x}q(\\text x,\\text y)-\\text{Weak Duality}\\)\nHence, in general we can say that the primal problem is greater than the dual problem. In case we want to optimize a function which is convex in nature than this does not hold true, in that case the primal problem is equal to the dual problem like the case in support vector machines.\n\\(min_{\\text x}max_{\\text y} q(\\text x,\\text y) = max_{\\text y}min_{\\text x}q(\\text x,\\text y)-\\text{Strong Duality (for convex functions)}\\)"
  },
  {
    "objectID": "posts/constraints.html#equality-constraints",
    "href": "posts/constraints.html#equality-constraints",
    "title": "Constraint Optimization",
    "section": "",
    "text": "When we want to let’ say optimize a function \\(f(\\text x)\\), where x could be a single value or a vector of n-dimensions, but we are subject to m equality constraint as follows\n\\(h_i(\\text x)=0 \\forall i \\in [1,..m]\\)\nLet’s better understand this with an example\nWe want to optimize the function \\(f(\\text x , \\text y)=x^2+y^2\\), given the equality constraint \\(h(\\text x , \\text y)=x+y-1\\). So basically we need values of x and y such that they optimize f while also satisfy h.\nFirst we need to figure out if this optimization problem is an maximizing problem or an minimizing problem. For that let’s plot f=1 and h.\n\n\n\n\n\n\n\n\n\n\nWe can see that if we keep increasing the value of f, we will always get intersection points with the line. Hence, f can go upto \\(\\infty\\). On the other hand if we decrease the value of f, we can only do it upto an extent or else f will not intersect with h and will not satisfy the constraint. Hence, this is an minimization problem. Clearly the solution to this problem will occur when we keep decreasing f until it becomes tangent to h.\nIf we consider the gradient of both functions, the gradient of the objective function is given by the red arrow which is the direction in which the function will increase the most with small change in inputs and the gradient of the constraint function is given by the blue arrow. Hence, we can conclude that when these 2 functions are tangent to each other which is the optimal case, \\(\\nabla f(\\text x^*,\\text y^*) = \\lambda \\nabla h(\\text x^*,\\text y^*)\\), their gradients at the optimal point are parallel. Here \\(\\text x^*,\\text y^*\\) are the optimal points.\n\n\nIn this method we construct a new function called the Lagrangian denoted by L.\n\\(\\text L(\\text x,\\text y,\\lambda)=f(\\text x,\\text y)+ \\lambda h(\\text x,\\text y)\\), \\(\\lambda\\) is called the lagrangian multiplier.\nWe then set \\(\\nabla L = 0\\) i.e \\(\\frac {\\partial \\text L}{\\partial \\text x}=0, \\quad \\frac {\\partial \\text L}{\\partial \\text y}=0, \\quad \\frac {\\partial \\text L}{\\partial \\lambda}=0\\)\nWe then solve for x, y and \\(\\lambda\\) using the three equations we get. Notice that \\(\\frac {\\partial \\text L}{\\partial \\lambda}=h(\\text x,\\text y)\\).Solving above, the three equations we get are\n\\[\\begin{equation}\\begin{split} 2\\text x&=\\lambda \\\\ 2\\text y&=\\lambda \\\\ \\text x + \\text y -\\text 1&=0 \\end{split}\\end{equation} \\]\nWe can see that since the equations are symmetric in x and y. Hence x must be equal to y = 0.5 and hence optimal solution is (0.5,0.5). Let’s visualize this."
  },
  {
    "objectID": "posts/constraints.html#kkt-conditions",
    "href": "posts/constraints.html#kkt-conditions",
    "title": "Constraint Optimization",
    "section": "",
    "text": "Our goal is to minimize f(x) such that,\n\\(\\begin{equation}\\begin{split}\\text h_i(\\text x)&=0,\\hspace{0.1cm} \\forall i=1,...m. \\\\ \\text g_i(\\text x)&&lt;=0,\\hspace{0.1cm} \\forall i=1,...n. \\end{split}\\end{equation}\\)\nHere as constraints we have m equalities and n inequalities. To solve this problem let’s first create the Lagragian function L as follows\n\\(\\text L(\\text x,\\lambda_1,...,\\lambda_m,\\mu_1,...,\\mu_n)\\hspace{0.1cm}=\\hspace{0.1cm}\\text f(\\text x)+\\sum_{i=1}^{m} \\lambda_i \\text h_i(\\text x)+\\sum_{j=1}^{n} \\mu_j \\text g_j(\\text x)\\)\nwhere, \\(\\lambda_1-\\lambda_m\\) are multipliers for the m equalities and \\(\\mu_1-\\mu_n\\) are multipliers for the n inequalities.\nWe then equate \\(\\nabla_\\text x \\text L(\\text x,\\lambda,\\mu)=0.\\) This condition is called the stationary condition which minimizes the objective function. We also equate \\(\\nabla_\\lambda \\text L(\\text x,\\lambda,\\mu)=0.\\) which is the equality condition.\nLet’s try to understand the inequality constraint with an example.\nLet f(x,y) = \\(\\text x^2 + \\text y^2\\) and let g(x) = \\(5-\\text x-\\text y\\) and g(x)\\(&lt;=0\\). Let’s consider f(x,y)=1.\n\n\n\n\n\nClearly we need to increase the value of f(x,y) such that it is atleast tangent to g(x)=0 to satisfy the constraint.\n\n\n\n\n\nNotice that in this optimum situation \\(\\mu g(\\text x,\\text y)=0\\) and we know that at point of tangency \\(g(\\text x,\\text y)=0\\). This conditon \\(\\mu g(\\text x,\\text y)=0\\) is called the complementary slackness condition and is always valid for all inequality constraints. If the optimal solution of the constraintless optimization already satisfies the constrainsts then in such as case \\(\\mu\\) for that constraint g is zero as applying that constraint doesn’t affect the solution. Note that even in this case \\(\\mu g(\\text x,\\text y)=0\\) is satisfied. Another constraint in the complementary slackness condition is that \\(\\mu_i &gt;=0\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n.\\) for the n inequalities.\nConsidering only 1 inequality constraint in an optimization problem. We know that \\(\\nabla \\text f=-\\mu \\nabla g\\). Hence, the value of \\(\\mu\\) is \\(\\frac {-\\nabla \\text f}{\\nabla \\text g}\\). As we can see in the plot above the gradients of f and g are in opposite directions and so the value of \\(\\mu= \\frac {-\\nabla \\text f}{\\nabla \\text g}\\) is non-negative.\nFor the above example, \\(\\nabla \\text f =\\hspace{0.1cm}-\\mu\\nabla \\text g , \\begin{bmatrix} \\text 2x \\\\ \\text 2y \\end{bmatrix}= -\\mu \\begin{bmatrix} \\text -1 \\\\ \\text -1 \\end{bmatrix}\\)\nHence,\n\\[\\begin{equation}\\begin{split} &\\text 2x=\\mu \\\\ &\\text 2y=\\mu \\\\ &5-x-y=0 \\end{split}\\end{equation}\\]\nBy symmetry of x and y we can say that x = y = 2.5 and the value of \\(\\mu\\) is 5 which is greater than zero.\nHence, we can summarize KKT conditions as follows\n\\(\\begin{equation}\\begin{split} &1.\\nabla_\\text x \\text L(\\text x,\\lambda,\\mu)=0. \\\\ &2.\\nabla_\\lambda \\text L(\\text x,\\lambda,\\mu)=0. \\\\ &3.\\mu_i &gt;=0\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n. \\\\ &4.\\mu_i g_i(\\text x,\\text y)=0.\\hspace{0.1cm}\\forall i\\hspace{0.1cm}=1,2,\\cdots n. \\end{split}\\end{equation}\\)\nKKT conditions are used in topics like ridge regression and support vector machines where there are constraints during optimization."
  },
  {
    "objectID": "posts/constraints.html#intuition-behind-lagrange-multipliers",
    "href": "posts/constraints.html#intuition-behind-lagrange-multipliers",
    "title": "Constraint Optimization",
    "section": "",
    "text": "Suppose we have an optimization problem, where we need to minimize \\(f(\\text x)\\), given inequality constraints \\(g_i \\le 0 \\hspace{0.1cm} \\forall i \\in [1,..,m]\\). The main idea is to include the constraints in the optimization function instead of the body of the optimization model. Hence, if we denote our objective function as \\(J(\\text x)\\),\n\\(J(\\text x)=f(\\text x)+\\sum_{i=1}^m P(g_i)\\), Here P is called a penalty function which is defined as \\(P(\\text x)= \\begin{cases} 0, \\text {if} \\hspace{0.1cm} \\text x \\le 0 \\\\ \\infty, \\text {if} \\hspace{0.1cm} \\text x \\gt 0 \\end{cases}\\)\nSo basically what the penalty function does is, if it’s input is a non-positive value (inequality constraint satisfied) then it output’s zero which is required as the penalty function musn’t influence the optimal value of x to minimize f. But if the constraint is not satisfied or the input to the penalty function is psotive then it results in infinity which increases the cost/objective function \\(J\\). Solving \\(J\\) is not feasible and difficult. Also, J is not a continuous function and hence not differentiable. To solve this problem we introduce lagrange variables/multipliers \\(\\mu_i \\hspace{0.1cm} \\forall i \\in [1,..m]\\). We define the Lagrange function \\(L\\) as follows which will help us approximate \\(J\\).\n\\(L(\\text x, \\mu)=f(\\text x)+\\sum_{i=1}^m \\mu_ig_i,\\hspace{0.1cm} \\mu_i \\ge 0\\). We have seen that if \\(\\mu_i=0\\) then \\(g_i\\) doesn’t affect the optimal value x.\nHow is \\(L \\hspace{0.1cm}\\text{and}\\hspace{0.1cm} J \\hspace{0.1cm}\\text {related?}\\)\n\\(J(\\text x)= \\max_{\\mu}L(\\text x,\\mu)\\) as largest possible value of \\(\\mu\\) is \\(\\infty\\).\nSo to solve the optimization problem we will now minimize \\(J(\\text x)\\) in terms of x. If \\(\\text x^*\\) is our optimal solution, then\n\\(\\text x^* = min_{\\text x}J(\\text x)\\hspace{0.1cm}=\\hspace{0.1cm}min_{\\text x}max_{\\mu}L(\\text x,\\mu)-\\text{primal problem}\\).\nLet’s try to solve this optimization problem using a different approach, let’s consider a function \\(L^*\\) such that\n\\(\\begin{equation}\\begin{split} &L^*(\\mu)=\\hspace{0.1cm}min_{\\text x}L(\\text x,\\mu) \\\\ &\\text x* = \\hspace{0.1cm} max_{\\mu}L^*(\\mu) \\\\ &\\text x*=\\hspace{0.1cm}max_{\\mu}min_{\\text x}L(\\text x,\\mu) -\\text {dual problem} \\end{split}\\end{equation}\\).\nHere we can use the concept of duality which is basically finding two different approches to solve the same problem. If we consider minimizing \\(f(\\text x)\\) in terms of x with constraints \\(g\\) to be our primal problem the maximizing the function \\(L^*(\\mu)\\) in terms of \\(\\mu\\) will be our dual problem.\nWe know from the minimax inequality that if we have a function let’s say \\(q(\\text x,\\text y)\\) then\n\\(min_{\\text x}max_{\\text y} q(\\text x,\\text y) \\ge max_{\\text y}min_{\\text x}q(\\text x,\\text y)-\\text{Weak Duality}\\)\nHence, in general we can say that the primal problem is greater than the dual problem. In case we want to optimize a function which is convex in nature than this does not hold true, in that case the primal problem is equal to the dual problem like the case in support vector machines.\n\\(min_{\\text x}max_{\\text y} q(\\text x,\\text y) = max_{\\text y}min_{\\text x}q(\\text x,\\text y)-\\text{Strong Duality (for convex functions)}\\)"
  },
  {
    "objectID": "posts/autodiff.html",
    "href": "posts/autodiff.html",
    "title": "Computational graphs and automatic differentiation",
    "section": "",
    "text": "Computational graphs\nComputational graphs is a method to represent complex mathematical expressions in simpler forms creating intermediate variables which simplify the expression. Given input values to the expression, computational graphs can calculate the final value of the graph by traversing the graph in the forward direction. They also provide a systematic way to compute derivatives of the final output of the graph with respect to the inputs and the intermediate variables by traversing the graph in the reverse direction. Hence, computational graphs can be useful in calculating gradient of a function and also help us understand automatic differentiation.\nLet’s better understand computational graphs with an example. Suppose we want to calculate the expression \\(\\text f(\\text x,\\text y)=\\log(\\text x^2 +\\sin(3\\text x \\text y))=\\text J\\).Here we are considering natural logarithm. We can create the intermediate variables as follows.\n\\[\\begin{equation}\\begin{split} \\text x,\\text y &= \\hspace{0.1cm} \\text{inputs} \\\\ \\text u &= \\hspace{0.1cm} 3\\text x\\text y \\\\ \\text v &= \\hspace{0.1cm} \\sin u \\\\ \\text z &= \\hspace{0.1cm} \\text v+\\text x^2 \\\\ \\text J &= \\hspace{0.1cm} \\log z \\\\ \\text J &= \\hspace{0.1cm} \\text{output} \\end{split}\\end{equation}\\]\nFollowing is the computational graph of the above function \\(\\text f\\).\n\n\nCode\nimport graphviz\n\ngraph = graphviz.Graph(graph_attr={'rankdir': 'LR'})\n\nwith graph.subgraph() as s:\n    s.node('x')\n    s.node('u=3xy',shape = 'rectangle')\n    s.node('v=sin(u)',shape = 'rectangle')\n    s.node('z=v+(x^2)',shape = 'rectangle')\n    s.node('J=log(z)',shape = 'rectangle')\n    s.node('output')\n\n\ngraph.node('y')\n\ngraph.edge('x', 'u=3xy', dir='forward')\ngraph.edge('y', 'u=3xy', dir='forward')\ngraph.edge('u=3xy', 'v=sin(u)', dir='forward')\ngraph.edge('v=sin(u)', 'z=v+(x^2)', dir='forward')\ngraph.edge('z=v+(x^2)', 'J=log(z)', dir='forward')\ngraph.edge('J=log(z)','output',dir='forward')\ngraph.edge('x', 'z=v+(x^2)', dir='forward')\n\n\ngraph\n\n\n\n\n\nLet’s take an example and compute the output by traversing the graph in the forward direction. We will take \\(\\text x =\\hspace{0.1cm} 2 \\hspace{0.1cm} \\text{and} \\hspace{0.1cm} \\text y = \\hspace{0.1cm}3\\).\nso \\(\\text f(2,3)=\\hspace{0.1cm} 1.178\\) rounded off to 3 decimal places. We can verify the same using computational graphs.\n\n\nCode\ngraph = graphviz.Graph(graph_attr={'rankdir': 'LR'})\n\nwith graph.subgraph() as s:\n    s.node('x')\n    s.node('u=3xy',shape = 'rectangle')\n    s.node('v=sin(u)',shape = 'rectangle')\n    s.node('z=v+(x^2)',shape = 'rectangle')\n    s.node('J=log(z)',shape = 'rectangle')\n    s.node('output')\n\n\ngraph.node('y')\n\ngraph.edge('x', 'u=3xy', dir='forward',label='2',fontcolor='green')\ngraph.edge('y', 'u=3xy', dir='forward',label='3',fontcolor='green')\ngraph.edge('u=3xy', 'v=sin(u)', dir='forward',label='18',fontcolor='green')\ngraph.edge('v=sin(u)', 'z=v+(x^2)', dir='forward',label='-0.75',fontcolor='green')\ngraph.edge('x', 'z=v+(x^2)', dir='forward',label='2',fontcolor='green')\ngraph.edge('z=v+(x^2)', 'J=log(z)', dir='forward',label='3.249',fontcolor='green')\ngraph.edge('J=log(z)','output',dir='forward',label='1.178',fontcolor='green')\n\n\n\ngraph\n\n\n\n\n\nComputational graphs can be used to calculate derivatives of final output of graph with respect to the inputs using the derivatives of the output with respect to the intermediate and the derivatives of the intermediate with respect to the inputs with the help of the chain rule. This is helpful in calculating the gradient of the output in the backpropagation algorithm which is used to train a neural network.\nWe can define \\(\\frac{\\partial \\text f}{\\partial \\text x}\\) as how much more is the change in \\(\\text f\\) when there is a small change in \\(\\text x\\) where \\(\\text f\\) is a function of \\(\\text x\\) and possibly more variables. For our example let’s consider the small change in \\(\\text x\\) as \\(\\Delta \\text x=\\hspace{0.1cm}0.00001\\).\nSo for example if we change \\(\\text x\\) by \\(\\Delta \\text x\\) and if \\(\\text f\\) changes by let’s say \\(0.00004\\), then \\(\\text f\\) has changed it’s value \\(4\\) times more than \\(\\text x\\) and hence, the value of derivative of \\(\\text f\\) with respect to \\(\\text x\\) is \\(4\\).\nFor our above example let’s try to compute the gradient of \\(\\text f\\) i.e \\(\\nabla \\text f = \\hspace{0.1cm} \\begin{bmatrix} \\frac{\\partial \\text f}{\\partial \\text x} \\\\ \\frac{\\partial \\text f}{\\partial \\text y} \\end{bmatrix}\\).\nLet’s change \\(\\text z\\) by \\(\\Delta \\text x\\). Hence \\(\\text z\\) is \\(3.24901\\). \\(\\text J\\) changes by \\(3.07\\text x10^{-6}\\) and hence change in \\(\\text J\\) by change in \\(\\text z\\) is \\(0.307\\) approximately which is the derivative of \\(\\text f\\) with respect to \\(\\text z\\). Ignoring the round offs this value is equal to \\(\\frac{1}{\\text z}\\).\nHence \\(\\frac{\\partial \\text J}{\\partial \\text z}=\\hspace{0.1cm} \\frac{1}{\\text z}=\\hspace{0.1cm}{0.307}\\)\n\n\nCode\ngraph = graphviz.Graph(graph_attr={'rankdir': 'LR'})\n\nwith graph.subgraph() as s:\n    s.node('x')\n    s.node('u=3xy',shape = 'rectangle')\n    s.node('v=sin(u)',shape = 'rectangle')\n    s.node('z=v+(x^2)',shape = 'rectangle')\n    s.node('J=log(z)',shape = 'rectangle')\n    s.node('output')\n\n\ngraph.node('y')\n\ngraph.edge('x', 'u=3xy', dir='forward',label='2',fontcolor='green')\ngraph.edge('y', 'u=3xy', dir='forward',label='3',fontcolor='green')\ngraph.edge('u=3xy', 'v=sin(u)', dir='forward',label='18',fontcolor='green')\ngraph.edge('v=sin(u)', 'z=v+(x^2)', dir='forward',label='-0.75',fontcolor='green')\ngraph.edge('x', 'z=v+(x^2)', dir='forward',label='2',fontcolor='green')\ngraph.edge('z=v+(x^2)', 'J=log(z)', dir='forward',label='3.249',fontcolor='green')\ngraph.edge('J=log(z)', 'z=v+(x^2)', dir='forward',label='0.307',fontcolor='red')\ngraph.edge('J=log(z)','output',dir='forward',label='1.178',fontcolor='green')\n\n\n\ngraph\n\n\n\n\n\nsimilarly we can calculate \\(\\frac{\\partial \\text z}{\\partial \\text v}\\) and that comes out to be \\(1\\) as \\(\\text z\\) increases by the same amount as \\(\\text v\\), and \\(\\frac{\\partial \\text v}{\\partial \\text u}\\) and that comes out to be \\(0.66\\) which is \\(\\cos u\\).\n\n\nCode\ngraph = graphviz.Graph(graph_attr={'rankdir': 'LR'})\nwith graph.subgraph() as s:\n    s.node('x')\n    s.node('u=3xy',shape = 'rectangle')\n    s.node('v=sin(u)',shape = 'rectangle')\n    s.node('z=v+(x^2)',shape = 'rectangle')\n    s.node('J=log(z)',shape = 'rectangle')\n    s.node('output')\n\n\ngraph.node('y')\n\ngraph.edge('x', 'u=3xy', dir='forward',label='2',fontcolor='green')\ngraph.edge('y', 'u=3xy', dir='forward',label='3',fontcolor='green')\ngraph.edge('u=3xy', 'v=sin(u)', dir='forward',label='18',fontcolor='green')\ngraph.edge('v=sin(u)', 'z=v+(x^2)', dir='forward',label='-0.75',fontcolor='green')\ngraph.edge('z=v+(x^2)','v=sin(u)', dir='forward',label='1',fontcolor='red')\ngraph.edge('x', 'z=v+(x^2)', dir='forward',label='2',fontcolor='green')\n# graph.edge('z=v+(x^2)','x', dir='forward',label='25.0074',fontcolor='red')\ngraph.edge('z=v+(x^2)', 'J=log(z)', dir='forward',label='3.249',fontcolor='green')\ngraph.edge('J=log(z)', 'z=v+(x^2)', dir='forward',label='0.307',fontcolor='red')\ngraph.edge('J=log(z)','output',dir='forward',label='1.178',fontcolor='green')\ngraph.edge('v=sin(u)','u=3xy',dir='forward',label='0.66',fontcolor='red')\n\n\n\ngraph\n\n\n\n\n\nIn the above graph, the red labels indicate the the derivative of the variable in the right box with respect to the variable in the left box. Hence, we know that,\n\\[\\begin{equation}\\begin{split} \\frac{\\partial \\text J}{\\partial \\text z}&=\\hspace{0.1cm}0.307 \\\\ \\frac{\\partial \\text z}{\\partial \\text v}&=\\hspace{0.1cm}1 \\\\ \\frac{\\partial \\text v}{\\partial \\text u}&=\\hspace{0.1cm}0.66 \\end{split}\\end{equation}\\]\nFrom chain rule, we can say that,\n\\[\\begin{equation}\\begin{split}\\frac{\\partial \\text J}{\\partial \\text v}&=\\hspace{0.1cm}\\frac{\\partial \\text J}{\\partial \\text z}.\\frac{\\partial \\text z}{\\partial \\text v} \\\\ \\frac{\\partial \\text J}{\\partial \\text v} &=\\hspace{0.1cm} 0.307 \\\\ \\frac{\\partial \\text J}{\\partial \\text u}&=\\hspace{0.1cm}\\frac{\\partial \\text J}{\\partial \\text v}.\\frac{\\partial \\text v}{\\partial \\text u} \\\\ \\frac{\\partial \\text J}{\\partial \\text u} &=\\hspace{0.1cm} 0.202  \\end{split}\\end{equation}  \\]\nIf we change \\(\\text y\\) by a small amount \\(0.00001\\) i.e from \\(3\\) to \\(3.00001\\) then \\(\\text u\\) changes by \\(0.00006\\) and hence the derivative of \\(\\text u\\) with respect to \\(\\text y\\) is equal to 6 which is equal to \\(3\\text x\\). Similarly derivative of \\(\\text u\\) with respect to \\(\\text x\\) is \\(9\\) which is \\(3\\text y\\). The derivative of \\(\\text z\\) with respect to \\(\\text x\\) is equal to \\(4\\), as if \\(\\text x\\) changes by \\(0.00001\\) then \\(\\text z\\) changes by \\(0.00004\\). These values are rounded off to two or three decimal places.\nHence,\n\n\nCode\ngraph = graphviz.Graph(graph_attr={'rankdir': 'LR'})\ngraph.node('x')\ngraph.node('y')\ngraph.node('u=3xy',shape = 'rectangle')\ngraph.node('v=sin(u)',shape = 'rectangle')\ngraph.node('z=v+(x^2)',shape = 'rectangle')\ngraph.node('J=log(z)',shape = 'rectangle')\ngraph.node('output')\n\ngraph.edge('z=v+(x^2)','x', dir='forward',label='4',fontcolor='red')\ngraph.edge('y','u=3xy',dir='forward',label='3',fontcolor='green')\ngraph.edge('u=3xy','y',dir='forward',label='6',fontcolor='red',constraint='false')\ngraph.edge('x','u=3xy',dir='forward',label='2',fontcolor='green')\ngraph.edge('u=3xy','x',dir='forward',label='9',fontcolor='red',constraint='false')\ngraph.edge('u=3xy','v=sin(u)',dir='forward',label='18',fontcolor='green')\ngraph.edge('v=sin(u)','z=v+(x^2)',dir='forward',label='-0.75',fontcolor='green')\ngraph.edge('x','z=v+(x^2)',dir='forward',label='2',fontcolor='green')\ngraph.edge('z=v+(x^2)','J=log(z)',dir='forward',label='3.249',fontcolor='green')\ngraph.edge('J=log(z)','output',dir='forward',label='1.178',fontcolor='green')\ngraph.edge('z=v+(x^2)','v=sin(u)', dir='forward',label='1',fontcolor='red')\ngraph.edge('J=log(z)', 'z=v+(x^2)', dir='forward',label='0.307',fontcolor='red')\ngraph.edge('v=sin(u)','u=3xy',dir='forward',label='0.66',fontcolor='red')\n\n\n\ngraph\n\n\n\n\n\n\\[ \\begin{equation}\\begin{split} \\frac{\\partial \\text J}{\\partial \\text y}&=\\hspace{0.1cm}\\frac{\\partial \\text J}{\\partial \\text u}.\\frac{\\partial \\text u}{\\partial \\text y} \\\\ \\frac{\\partial \\text J}{\\partial \\text y}&=\\hspace{0.1cm}1.22 \\\\ \\frac{\\partial \\text J}{\\partial \\text x}&=\\hspace{0.1cm}\\frac{\\partial \\text u}{\\partial \\text x}.\\frac{\\partial \\text J}{\\partial \\text u} + \\frac{\\partial \\text J}{\\partial \\text z}.\\frac{\\partial \\text z}{\\partial \\text x} \\\\ \\frac{\\partial \\text J}{\\partial \\text x}&=\\hspace{0.1cm}3.05 \\end{split}\\end{equation}\\]\nHence, if \\(\\text x=\\hspace{0.1cm}2\\) and \\(\\text y=\\hspace{0.1cm}3\\), then \\(\\frac{\\partial \\text J}{\\partial \\text y}=\\hspace{0.1cm}1.22 \\hspace{0.2cm} \\text{and}\\hspace{0.1cm}\\frac{\\partial \\text J}{\\partial \\text x}=\\hspace{0.1cm}3.05\\) approximately, rounded off to two decimal places.\n\n\nAutomatic differentiation in python\nWhen we train neural netwworks for practical applications, they sometimes have millions of parameters and we need to calculate the derivative of the cost function with respect to each of these parameters with the help of intermediate variables and chain rule as seen above. So with the help of computer programs we can calculate gradient of the cost function efficiently.\nThere are mainly two types of automatic differentiations, namely:\n\nForward mode\nReverse mode\n\n\nForward mode\nIn the forward mode we calculate the values of the intermediate variables also called the primals as we traverse through the graph, but simultaneously we also calculate the values of the derivatives(tangents) of the primals with respect to the input variables. We have to run a seperate forward pass for each input variable.\nLet’s perform automatic differentiation (forward mode) in python from scratch\n\n\nCode\nclass Pair:\n    def __init__(self, val, der,var):\n        self.val = val\n        self.der = der\n        self.var = var\n\n    def __add__(self, other):\n        if isinstance(other, Pair):\n            return Pair(self.val + other.val, self.der + other.der,self.var)\n        else:\n            return Pair(self.val + other, self.der,self.var)\n\n    def __sub__(self, other):\n        if isinstance(other, Pair):\n            return Pair(self.val - other.val, self.der - other.der,self.var)\n        else:\n            return Pair(self.val - other, self.der,self.var)\n\n    def __mul__(self, other):\n        if isinstance(other, Pair):\n            return Pair(self.val * other.val,  other.val,self.var)\n        else:\n            return Pair(self.val * other, self.der * other,self.var)\n\n    def __truediv__(self, other):\n        if isinstance(other, Pair):\n            return Pair(self.val / other.val, (self.der * other.val - self.val * other.der) / (other.val ** 2),self.var)\n        else:\n            return Pair(self.val / other, self.der / other,self.var)\n\n    def sin(self):\n        return Pair(np.sin(self.val), self.der * np.cos(self.val),self.var)\n\n    def log(self):\n        return Pair(np.log(self.val), self.der / self.val,self.var)\n\n    def __pow__(self, power):\n        return Pair(self.val ** power, power * self.val ** (power - 1) * self.der,self.var)\n\n    def __repr__(self):\n        return f\"Pair of function value and derivative with respect to {self.var} is ({round(self.val,3)}, {round(self.der,2)})\"\n\npairx = Pair(2,1,\"x\")\npairy = Pair(3,1,\"y\")\n\ndef forward_pass_x(pairx,fv):\n    u = (pairx*fv)*3\n    v = u.sin()\n    z = v + pairx**2\n    J = z.log()\n    print(J)\n\ndef forward_pass_y(pairy,fv):\n    u = (pairy*fv)*3\n    v = u.sin()\n    z = v + fv**2\n    J = z.log()\n    print(J)\n    \nforward_pass_x(pairx,3)\nforward_pass_y(pairy,2)\n\n\nPair of function value and derivative with respect to x is (1.178, 3.06)\nPair of function value and derivative with respect to y is (1.178, 1.22)\n\n\n\n\nReverse mode\nReverse mode autodiff is the same process as what we did in the above example in computation graphs. We forward pass through the computation graph calculating the final output and then we reverse pass through the graph calculating the derivative of the final output with respect to the intermediate variables and then using chain rule to calculate the derivative of the final output with respect to the input variables. Unlike forward mode autodiff we can calulate the gradient of the output function in just one reverse paas through the computation graph.\nLet’s implement reverse mode automatic differentiation from scratch and then verify it using pytorch.\n\n\nCode\nimport numpy as np\nclass Scalar:    \n    def __init__(self, val):\n        self.val = val\n        self.grad = 0.\n        self.backward = lambda: None\n        \n    def __repr__(self):\n        return f\"Value: {self.val}, Gradient: {self.grad}\"\n    \n    def __add__(self, other):\n        if isinstance(other,Scalar):\n            result = Scalar(self.val + other.val)\n            def backward():\n                self.grad += result.grad\n                other.grad += result.grad\n                self.backward()\n                other.backward()\n            result.backward = backward\n        else:\n            result = Scalar(self.val+other)\n            def backward():\n                self.grad += result.grad\n                self.backward()\n            result.backward = backward\n        return result\n\n    def __mul__(self, other):\n        if isinstance(other,Scalar):\n            result = Scalar(self.val * other.val)\n            def backward():\n                self.grad += other.val * result.grad\n                other.grad += self.val * result.grad\n                self.backward()\n                other.backward()\n            result.backward = backward\n        else:\n            result = Scalar(self.val*other)\n            def backward():\n                self.grad += other * result.grad\n                self.backward()\n            result.backward = backward\n        return result\n\n    def sin(self):\n        result = Scalar(np.sin(self.val))\n        def backward():\n            self.grad += np.cos(self.val)*result.grad\n            self.backward()\n        result.backward = backward\n        return result\n    def log(self):\n        result = Scalar(np.log(self.val))\n        def backward():\n            self.grad += (1/(self.val))*result.grad\n            self.backward()\n        result.backward = backward\n        return result\nx = Scalar(2.0)\ny = Scalar(3.0)\nu = x*y*3\nv = u.sin()\nz = v + (x*x)\nJ = z.log()\nJ.grad = 1.\nJ.backward()\nprint(\"Rounding off the derivatives to two decimal places\")\nprint(\"Derivative of J wrt x\",round(x.grad,2))\nprint(\"Derivative of J wrt y\",round(y.grad,2))\n\n\nRounding off the derivatives to two decimal places\nDerivative of J wrt x 3.06\nDerivative of J wrt y 1.22\n\n\nLet’s now use pytorch\n\n\nCode\nimport torch\n\nx = torch.tensor(2.0,requires_grad=True)\ny = torch.tensor(3.0,requires_grad=True)\n\ndef J(x,y):\n    u = 3*x*y\n    v = torch.sin(u)\n    z = v + x**2\n    return torch.log(z),u,v,z\noutput,u,v,z = J(x,y)\nprint(\"derivative of output wrt u is\",torch.autograd.grad(output,u,retain_graph=True)[0].item())\nprint(\"derivative of output wrt v is\",torch.autograd.grad(output,v,retain_graph=True)[0].item())\nprint(\"derivative of output wrt z is\",torch.autograd.grad(output,z,retain_graph=True)[0].item())\nprint()\nprint(\"output is\",round(output.item(),3),\"rounded off to three decimal places\")\nprint(\"derivative of output wrt x is\",round(torch.autograd.grad(output,x,retain_graph=True)[0].item(),2),\"rounded off to two decimal places\")\nprint(\"derivative of output wrt y is\",round(torch.autograd.grad(output,y)[0].item(),2),\"rounded off to two decimal places\")\n\n\nderivative of output wrt u is 0.20323611795902252\nderivative of output wrt v is 0.3077858090400696\nderivative of output wrt z is 0.3077858090400696\n\noutput is 1.178 rounded off to three decimal places\nderivative of output wrt x is 3.06 rounded off to two decimal places\nderivative of output wrt y is 1.22 rounded off to two decimal places\n\n\nThe reason automatic differentiation is faster than let’s say symbolic differentiation although they have similar accuracy is because the sole purpose of autodiff is to calculate the numerical value of the derivative by performing underlying primitive operations whose derivative we know for eg derivative of \\(\\sin x\\) is \\(\\cos x\\). We simplify the function with the help of different intermediate variables only consisting such primitive operations whose derivative we know. In symbolic differentiation there is the possibility of repeated calculations which lowers it’s efficiency."
  }
]