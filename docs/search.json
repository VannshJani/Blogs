[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Taylor_series/index (5).html",
    "href": "posts/Taylor_series/index (5).html",
    "title": "Taylor series",
    "section": "",
    "text": "The Taylor series is a mathematical representation of a function as an infinite sum of terms. It is mainly used to approximate a non-polynomial function in terms of polynomials of degree n. Higher the degree n, the better the approximation.\nThe general form of the Taylor series for the function \\(f(x)\\) centered about the point a is given by:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} +...\n\\]\nLet’s say we take the function \\(y = sin(x)\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-63, 64)]\ny = [math.sin(xi) for xi in x]\n\n\nplt.plot(x, y,)\nplt.title('Plot of y = sin(x)')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.show()\n\n\n\n\nLet’s choose a=0 and let’s go up to n=5, then using the Taylor series we can write,\n\\[\nsin(x) = 0 + cos(0)(x)+\\frac{-sin(0)x^2}{2!}+\\frac{-cos(0)x^3}{3!}+\\frac{sin(0)x^4}{4!}+\\frac{cos(0)x^5}{5!}\n\\]\nwhere \\(f(x)=sin(x),f'(x)=cos(x),f''(x)=-sin(x),f'''(x)=-cos(x),f''''(x)=sin(x),and f'''''(x)=cos(x)\\)\nTherefore,\n\\(sin(x) = x-\\frac{x^3}{3!}+\\frac{x^5}{5!}\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-33, 34)]\ny = [xi-((xi**3)/6)+((xi**5)/120) for xi in x]\ny1 = [math.sin(xi) for xi in x]\nplt.figure()\nplt.plot(x, y,label=\"y=x-(x^3)/3!+(x^5)/5!\")\nplt.plot(x,y1,label=\"y=sinx\")\nplt.title('Approximating sin(x) using y = x - (x^3)/3! + (x^5)/5!')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nWe can see above that, the polynomial function is providing us a good approximation of the sine function but only for values close to zero (as we selected a to be 0). For other values the approximation is inaccurate.\nWhen we truncate the infinite Taylor series to a polynomial of degree n, we call it the Taylor polynomial of degree n. The higher the value of n, the more accurate our approximation is."
  },
  {
    "objectID": "posts/Taylor_series/index (5).html#what-is-the-taylor-series",
    "href": "posts/Taylor_series/index (5).html#what-is-the-taylor-series",
    "title": "Taylor series",
    "section": "",
    "text": "The Taylor series is a mathematical representation of a function as an infinite sum of terms. It is mainly used to approximate a non-polynomial function in terms of polynomials of degree n. Higher the degree n, the better the approximation.\nThe general form of the Taylor series for the function \\(f(x)\\) centered about the point a is given by:\n\\[\nf(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} +...\n\\]\nLet’s say we take the function \\(y = sin(x)\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-63, 64)]\ny = [math.sin(xi) for xi in x]\n\n\nplt.plot(x, y,)\nplt.title('Plot of y = sin(x)')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.show()\n\n\n\n\nLet’s choose a=0 and let’s go up to n=5, then using the Taylor series we can write,\n\\[\nsin(x) = 0 + cos(0)(x)+\\frac{-sin(0)x^2}{2!}+\\frac{-cos(0)x^3}{3!}+\\frac{sin(0)x^4}{4!}+\\frac{cos(0)x^5}{5!}\n\\]\nwhere \\(f(x)=sin(x),f'(x)=cos(x),f''(x)=-sin(x),f'''(x)=-cos(x),f''''(x)=sin(x),and f'''''(x)=cos(x)\\)\nTherefore,\n\\(sin(x) = x-\\frac{x^3}{3!}+\\frac{x^5}{5!}\\)\n\nimport math\nimport matplotlib.pyplot as plt\n\nx = [i * 0.1 for i in range(-33, 34)]\ny = [xi-((xi**3)/6)+((xi**5)/120) for xi in x]\ny1 = [math.sin(xi) for xi in x]\nplt.figure()\nplt.plot(x, y,label=\"y=x-(x^3)/3!+(x^5)/5!\")\nplt.plot(x,y1,label=\"y=sinx\")\nplt.title('Approximating sin(x) using y = x - (x^3)/3! + (x^5)/5!')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\nWe can see above that, the polynomial function is providing us a good approximation of the sine function but only for values close to zero (as we selected a to be 0). For other values the approximation is inaccurate.\nWhen we truncate the infinite Taylor series to a polynomial of degree n, we call it the Taylor polynomial of degree n. The higher the value of n, the more accurate our approximation is."
  },
  {
    "objectID": "posts/Taylor_series/index (5).html#taylor-series-for-bivariate-functions",
    "href": "posts/Taylor_series/index (5).html#taylor-series-for-bivariate-functions",
    "title": "Taylor series",
    "section": "Taylor series for bivariate functions",
    "text": "Taylor series for bivariate functions\nWe can generalize the Taylor series for functions depending on multiple variables.\nLet’s discuss the Taylor series for bivariate functions. The formula is given by,\n\\[\nf(x,y)= f(a,b)+\\frac{\\partial f}{\\partial x}(x-a)+\\frac{\\partial f}{\\partial y}(y-b)+\\frac{\\partial^2 f}{\\partial x^2}\\frac{(x-a)^2}{2!}+\\frac{\\partial^2 f}{\\partial y^2}\\frac{(y-b)^2}{2!}+...\n\\]\nHere the approximation of the function takes place near the point,\\((a,b)\\), and the partial derivatives are calculated at the point \\((a,b)\\).\nLet’s take an example.\n\\[\nf(x,y)=x^2+y^2\n\\]\nWe can try to approximate this function using the 2D Taylor series around the point \\((1,1)\\) and let’s choose the value of n as 2.\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx1 = np.linspace(-5,5,100)\ny1 = np.linspace(-5,5,100)\nX1,Y1 =np.meshgrid(x1,y1)\nZ_true=X1**2 + Y1**2\n\nfig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.plot_surface(X1,Y1,Z_true,cmap=\"plasma\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nax.view_init(elev=20, azim=30)\nplt.show()\n\n\n\n\nLet us try to approximate this function using the Taylor series with a=b=1 and n =2.\n\\[\n\\begin{equation} \\begin{split}\nf(x,y)&=2+2(x-1)+2(y-1)+(x-1)^2+(y-1)^2\\\\\nf(x,y)&=x^2+y^2 \\end{split} \\end{equation}\n\\]\nwhich is the true value indeed. Hence, the Taylor series exactly approximates \\(f(x,y)\\) when centered about the point \\((1,1)\\).\nYou can visualize the approximations of the Taylor series in the link given below.\nhttps://vannshtaylor.streamlit.app/"
  },
  {
    "objectID": "posts/Eigenvalue/index (6).html",
    "href": "posts/Eigenvalue/index (6).html",
    "title": "Matrix Decompositions",
    "section": "",
    "text": "1. Eigenvalue Decomposition\nMatrix decompositions are needed to solve large system of linear equations or diagonalize a matrix on a computer. Eigenvalue decomposition of a square matrix is factorizing it into a product of three matrices representing it’s eigenvectors and eigenvalues.\n\nLet’s say we take a NxN shaped matrix A and we want to decompose it.\nwe consider a matrix U such that all the eigenvectors of A are the column vectors of U.\nThen we can say\n\\[   U=\\begin{bmatrix} u_1 & u_2 \\cdots&u_n  \\end{bmatrix}  \\]\nwhere \\(u_1,u_2\\cdots u_n\\) are eigenvectors of A.\nIf we post-multiply A with U we get,\n\\[   \\begin{equation} \\begin{split} AU &= \\begin{bmatrix} Au_1 & Au_2 \\cdots&Au_n  \\end{bmatrix} \\\\ &= \\begin{bmatrix} \\lambda_1 u_1 & \\lambda_2u_2 \\cdots&\\lambda_nu_n  \\end{bmatrix} \\end{split} \\end{equation}  \\]\nas \\(u_i\\) is the eigenvector of A \\(\\forall \\quad i \\in[1,..n]\\quad Au_i=\\lambda_iu_i\\), where \\(\\lambda_i\\) is the corresponding eigenvalue.\nNext, we can split the RHS into U and a diagonal matrix containing the eigenvalues of A\\((\\Lambda)\\). \\[\\begin{equation} \\begin{split} AU &=\\begin{bmatrix} u_1 & u_2 \\cdots&u_n  \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 \\cdots& 0 \\\\ 0 & \\lambda_2 \\cdots& 0 \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ 0 & 0 \\cdots& \\lambda_n \\end{bmatrix} \\\\ AU &= U\\Lambda \\end{split} \\end{equation}\\]\nNow, we know that U is an invertible matrix(column vectors of U are linearly independant as they are the eigenvectors of A) hence, we can post-multiply with \\(U^{-1}\\) on both sides.\n\\[  A=U\\Lambda U^{-1}  \\]\nThe eigenvalue decomposition of A is hence complete.\nThere is a special case to the above scenario. If A is a symmetric matrix, then it’s eigenvalues are real and it’s eigenvectors are orthogonal\\((i.e. v_i.v_j=0 \\quad where \\quad i \\not=j)\\). Further, if the eigenvectors are also orthonormal\\((i.e. v_i.v_i=1 \\quad where \\quad i \\in[1,..n])\\), then we can conclude that,\n\\[   Q=U^TU=I  \\]and hence to calculate the inverse of a symmetric matrix, we can calculate it’s transpose which reduces the complexity by n-fold.\nMoreover, we can generalize to higher powers of A,\n\\[  \\begin{equation} \\begin{split} A &= U\\Lambda U^{-1} \\\\ A^2 &= U\\Lambda^2 U^{-1} \\\\ \\vdots \\\\ A^n &= U\\Lambda^n U^{-1} \\end{split} \\end{equation}  \\]\nIf we have to compute higher powers of A, we can use the right hand side of the above equation to compute the higher power of \\(\\Lambda\\) which is a diagonal matrix. The higher power of a diagonal matrix can be calculated by raising the diagonal elements to that power, and thus, eigenvalue deconposition can reduce the computational cost.\nLet’s now apply eigenvalue decomposition to a matrix."
  },
  {
    "objectID": "posts/shannonfano/index 2 (2).html",
    "href": "posts/shannonfano/index 2 (2).html",
    "title": "Shannon Fano encoding",
    "section": "",
    "text": "Shannon Fano encoding\nThe Shannon fano algorithm is a lossless data compression technique. Data compression is the process of encoding or converting data in such a way that it consumes less memory space. Hence, it reduces the resources required to store and transmit data.\nFor eg, it would make more sense to assign symbols or characters occuring more frequently to consume less number of bits than to assign equal bits to all characters. Hence, it assigns codes of variable lengths to each character. In the realm of data compression, two distinct but connected methods for creating a prefix code for the characters is Shannon Fano encoding.\n\n\nAlgorithm\n\nWe find out the frequency or probability of occurance of each character or symbol and store it in an array.\nWe sort the array in decreasing order based on probability(frequency).\nThen, we split the list in two parts with as much similarity(least variance) between the frequency/probability of left and right parts.\nWe assign the binary digit ‘0’ to the left part and binary digit ‘1’ to the right part. This implies that symbols in the left section will begin with ‘0’ and symbols in the right section will begin with ‘1’.\nWe repeat steps 3 and 4 by splitting each section into left and right sections and assigning the binary digits ‘0’ and ‘1’ to them unitl each section is left with just one symbol(analogous to leaf node of binary tree).\n\nLet’s better understand shannon-fano and visualize it using examples. We’ll encode the characters of the following sentence.\n“In the tranquil stillness of the early morning, birdsong filled the air as the first rays of sunlight painted the horizon with hues of pink and orange, creating a breathtakingly beautiful scene that seemed to belong to a world untouched by time.”\nLet’s first calculate the probability of each character in the above statement, and sort them based on probabilities in descending order.\n\nsentence = \"aaaaaaaabbbbccd\"\ndef generate_prob(sentence):\n    d={}\n    special=0\n    for c in sentence:\n        if c==\",\" or c==\".\" or c==\" \":\n            special+=1\n            continue\n        elif c.lower() in d:\n            d[c.lower()] += 1\n        else:\n            d[c.lower()] = 1\n    total_char = len(sentence)-special\n    for key in d:\n        d[key] = d[key]/total_char\n    return d\nseq = generate_prob(sentence)\nseq=sorted(seq.items(),key=lambda item: item[1], reverse=True)\nd = dict(seq)\nd\n\n{'a': 0.5333333333333333,\n 'b': 0.26666666666666666,\n 'c': 0.13333333333333333,\n 'd': 0.06666666666666667}\n\n\nWe create nodes for the sequence and build a tree following the top to bottom approach.\nSince we want the splits to have similar total probabilities, we can assign ‘0’ and ‘1’ alternatively on the descended probability sequence or we can create the left subset until the probabilities of all the subsets approximately reach half of the total probability value.\n\nclass Node:\n    def __init__(self,char,prob):\n        self.char=char\n        self.p=prob\n        self.code=\"\"\n        self.left=None\n        self.right=None\n\ninitial=\"\"\nfor c in seq:\n    initial+=c[0]\nroot = Node(initial,1)\n\n\ndef split(node):\n    left_subset={}\n    right_subset={}\n    total_p = node.p\n    half_p = total_p/2\n    sum=0\n    index = 0\n    seq=node.char\n    for c in seq:\n        if sum&gt;=half_p:\n            break\n        sum+=d[c]\n        index+=1\n    left_char = seq[:index]\n    right_char = seq[index:]\n    for l in left_char:\n        left_subset[l]=d[l]\n    for r in right_char:\n        right_subset[r]=d[r]\n    return left_subset,right_subset\n\ndef Build_tree(root):\n    if len(root.char)==1:\n        return\n    left_subset,right_subset = split(root)\n    left_char=\"\"\n    right_char=\"\"\n\n    for c in left_subset:\n        left_char+=c\n    for cr in right_subset:\n        right_char+=cr\n    left_prob = sum(left_subset.values())\n    right_prob = sum(right_subset.values())\n\n    left_node = Node(char=left_char,prob=left_prob)\n    right_node = Node(char=right_char,prob=right_prob)\n    root.left = left_node\n    root.right = right_node\n    Build_tree(left_node)\n    Build_tree(right_node)\n    \nBuild_tree(root)\n\ndef assign_codes(root):\n    if root.left is None and root.right is None:\n        return\n    if root.left is not None:\n        root.left.code = \"0\" + root.code\n    if root.right is not None:\n        root.right.code = \"1\" + root.code\n    assign_codes(root.left)\n    assign_codes(root.right)\nassign_codes(root)\n\ntotal_bits=0\ndef print_codes(root):\n    global total_bits\n    if root is not None:\n        if (len(root.char)==1):\n          total_bits += len(root.code)*sentence.count(root.char)\n          print(f\"Character is '{root.char}', probability is {round(root.p,4)} and code is {root.code}\")\n        print_codes(root.left)\n        print_codes(root.right)\nprint_codes(root)\n\nCharacter is 'a', probability is 0.5333 and code is 0\nCharacter is 'b', probability is 0.2667 and code is 01\nCharacter is 'c', probability is 0.1333 and code is 011\nCharacter is 'd', probability is 0.0667 and code is 111\n\n\nHence, we follow the top-bottom approach, where we add ‘0’ to the left subset and ‘1’ to the right subset as we traverse down the tree.\n\n\nVisualizing the Binary Tree\n\n\n\n\n\n\n\nResults and Observations\nIf we consider the “ascii” encoding, we would have to consider 8 bits for each letter, whereas we consume less number of bits using shannon-fano encoding.\n\nprint(f\"No.of bits using ascii = {len(sentence)*8}\")\nprint(f\"No.of bits using shannon-fano = {total_bits}\")\n\nNo.of bits using ascii = 120\nNo.of bits using shannon-fano = 25\n\n\nShannon-fano algorithm certainly performs better than the ascii encoding and does a decent job.\n\nCompression ratio\nThe compression ratio tells us how much smaller the compressed data is compared to the original data. The higher the compression ratio, the better the compression.\nCompression ratio = Before Compression / After compression\n\nb = len(sentence)*8\na = total_bits\nprint(f\"compression ratio is,\",(b/a))\n\ncompression ratio is, 4.8\n\n\n\n\nEntropy\nEntropy is the measure of randomness of data, higher the entropy more random the data. More random the data, the more number of bits we need to measure it.\n\\[\nH(x) = -\\Sigma_{i=1}^n p(x_i)log_2(p(x_i))\n\\]\n\nimport numpy as np\ndef calc_entropy(p):\n  return -1 * p * np.log2(p)\n\nentropy = 0\nfor c in d:\n  entropy += calc_entropy(d[c])\n  \nprint(f\"Entropy of input data is {entropy}\")\n\nEntropy of input data is 1.640223928941852\n\n\nIt turns out, shannon fano algorithm is not the most optimized algorithm and may not work always. The Huffman encoding which is a slightly modified version of the shannon-fano optimizes the data compression even further.\nhttps://vannshsf.streamlit.app/"
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html",
    "href": "posts/2ndorder/index(2) (1).html",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "Hessian matrix is a square matrix of second order partial derivatives of a scalar valued function. It can be used to describe the local curvature of the function at a point and it is denoted by H.\n\\[\nH(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\cdots& \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\cdots& \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &\\quad \\vdots \\quad \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}  \\cdots& \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\n\\]\nFor example, let’s take a bivariate function(n=2),\n\\[\nf(x,y) = xy^2\n\\]\n\\[\nH(f)= \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}\n\\]\nHere, \\(\\frac{\\partial f}{\\partial x}=y^2\\), \\(\\frac{\\partial f}{\\partial y}=2xy\\), \\(\\frac{\\partial^2 f}{\\partial x^2}=0,\\frac{\\partial^2 f}{\\partial y^2}=2x,\\frac{\\partial^2 f}{\\partial x \\partial y}=\\frac{\\partial^2 f}{\\partial y \\partial x}=2y\\)\n\\[\nH(f)=\\begin{bmatrix} 0 & 2y \\\\ 2y & 2x \\end{bmatrix}\n\\]\nUsing this matrix, we can find out the nature of the curvature at any point \\((x_1,y_1)\\), by substituting this point in the Hessian.\nIf the Hessian matrix is positive definite(all eigenvalues are positive) at a point, it indicates that the function is locally convex(has a local minimum) around that point. If it is negative definite, the function is locally concave. If the eigenvalues have both positive and negative values, then this point has a mixture of concave and convex behaviour in different directions and such a point is called a saddle point.\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a sample function\ndef f(x):\n    return x[0]*x[1]**2 \n\n# point where you want to compute Hessian matrix\n# requires_grad=True tells pytorch to keep track of x0 which form a computation graph to compute gradients easily.\nx0 = torch.tensor([2.0, 1.0], requires_grad=True)\n# create_graph=True is used to compute higher order derivatives in the computation graph\ngrads = torch.autograd.grad(f(x0), x0, create_graph=True)[0]\nHessian = torch.zeros((len(x0), len(x0)))\nfor i in range(len(x0)):\n    Hessian[i] = torch.autograd.grad(grads[i], x0, retain_graph=True)[0]\n\nHessian = Hessian.detach().numpy()\nplt.imshow(Hessian, cmap='coolwarm')\nplt.xticks(np.arange(len(x0)))\nplt.yticks(np.arange(len(x0)))\nplt.xlabel('Hessian Row Index')\nplt.ylabel('Hessian Column Index')\nplt.colorbar()\nplt.title('Visualization of the Hessian Matrix')\nplt.show()\n\n\n\n\n\n\nFollowing are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5718e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses,X,Y,Z):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Initial point\")\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Final point\")\n  plt.legend()\n  plt.show()\n  \nplot_contour(guesses,X,Y,Z)\n\n\n\n\n\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess. Depending upon different initial guesses, the final guess could land onto different minimasor possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses,X,Y,Z)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it.\n\n\n\nThe BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n/var/folders/76/s25grzw958ld2_m4fhl5j5900000gn/T/ipykernel_49912/298132734.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n  Z = f(torch.tensor([X, Y])).detach().numpy()\n\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima.\n\n\n\nThe LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "href": "posts/2ndorder/index(2) (1).html#newtons-method-for-optimizing-bivariate-functions-using-hessian.",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "Following are the steps to find minimum or maximum of a function:\n\nMake an intial guess.\nAt the initial guess, we find out how steep the slope of the curve is and how quickly the slope is changing. Hence, we calculate the first derivative and the second derivative at this point.\nWe can approximate a quadratic function(parabolic bowl) at that point using the taylor series.\nNewton’s method then moves to the minimum of the parabolic bowl which is the new guess for the optimal point of the original function.\nThis process repeats and with each iteration you edge closer to the optimal value of the original function and finally newton’s method converges.\n\nAt any iteration, the value of \\(x\\) can be updated as,\n\\[\nx_{i+1} = x_i-H^{-1}\\nabla f(x_i) \\quad -(*)\n\\]\nwhere \\(H^{-1}\\) is the inverse of the hessian(which is initially assumed to be the identity matrix) and \\(\\nabla f(x_i)\\) is an array/vector containing the partial derivatives of \\(f\\) with respect to all the variables.\nFollowing is the code for optimizing \\(f(x,y)=-sin(x)-cos(y)\\).\nLet’s first plot \\(f(x,y)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f(X,Y)\n\nfig = plt.figure(figsize=(8,6))\nax1= fig.add_subplot(111, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nax1.set_title('f(x) = -sin(x) - cos(y)')\nplt.show()\n\n\n\n\n\nimport torch\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\niterations = 10\n\n\ndef newton(guess,f,iterations):\n  guesses=[]\n  guesses.append(guess)\n  for i in range(iterations):\n      f_value = f(guess)\n      gradient = torch.autograd.grad(f_value, guess, create_graph=True)[0]\n      hessian = torch.zeros((len(guess), len(guess)))\n      for j in range(len(guess)):\n          hessian_row = torch.autograd.grad(gradient[j], guess, retain_graph=True)[0]\n          hessian[j] = hessian_row\n      step = -torch.linalg.solve(hessian, gradient)\n      guess = guess + step\n      guesses.append(guess)\n  return guesses\n      \n\nguess = torch.tensor([2.0, 1.0], requires_grad=True)\nguesses=newton(guess,f,iterations)\nfor i in range(len(guesses)):\n  print(f\"Iteration {i}: guess = {guesses[i]}\")\n\nIteration 0: guess = tensor([2., 1.], requires_grad=True)\nIteration 1: guess = tensor([ 1.5423, -0.5574], grad_fn=&lt;AddBackward0&gt;)\nIteration 2: guess = tensor([1.5708, 0.0659], grad_fn=&lt;AddBackward0&gt;)\nIteration 3: guess = tensor([ 1.5708e+00, -9.5718e-05], grad_fn=&lt;AddBackward0&gt;)\nIteration 4: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 5: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 6: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 7: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 8: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 9: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\nIteration 10: guess = tensor([1.5708, 0.0000], grad_fn=&lt;AddBackward0&gt;)\n\n\nSo after updating our guess using \\((*)\\) for a sufficient number of iterations, we get our final guess as \\(x=1.5708 \\quad and \\quad y=0.0\\).\nLet’s plot the contour plot of the above function to verify our results.\n\nimport time\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-6, 6, 100)\ny = np.linspace(-6, 6, 100)\nX, Y = np.meshgrid(x, y)\n\ndef f1(X,Y):\n    return -np.sin(X) - np.cos(Y)\nZ = f1(X,Y)\n\ndef plot_contour(guesses,X,Y,Z):\n  fig=plt.figure(figsize=(10,6))\n  ax = fig.add_subplot(111)\n  contour = ax.contourf(X,Y,Z)\n  plt.colorbar(contour)\n  ax.set_xlabel(\"X\")\n  ax.set_ylabel(\"Y\")\n  ax.set_title(\"Contour plot of f(x,y)\")\n  marker=\"\"\n  for i in range(2):\n    if i==0:\n      marker=\"o\"\n      color=\"cyan\"\n      ax.scatter(guesses[0][0].detach().numpy(), guesses[0][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Initial point\")\n    else:\n      marker=\"x\"\n      color=\"red\"\n      ax.scatter(guesses[-1][0].detach().numpy(), guesses[-1][1].detach().numpy(), color=color, alpha=1,marker=marker,label=\"Final point\")\n  plt.legend()\n  plt.show()\n  \nplot_contour(guesses,X,Y,Z)\n\n\n\n\n\nThrough the contour plot we can understand that even though our initial guess was the point \\((2,1)\\) we finally reached the minima of the function. In the above contour plot, the bluish circle is the initial guess and the red cross is the final guess. Depending upon different initial guesses, the final guess could land onto different minimasor possibly even a saddle point.\nLet’s say we take another point \\((1,-2)\\).\n\nguess = torch.tensor([1.0, -2.0], requires_grad=True)\niterations=10\nguesses = newton(guess,f,iterations)\nplot_contour(guesses,X,Y,Z)\n\n\n\n\nIn the case above we got a saddle point, this is one of the drawbacks of the newton method.\nAlthough the newton’s method for optimization converges faster than the gradient descent algorithm and one doesn’t have to also face the difficulty in deciding the learning rate as is faced in gradient descent, the computation of the Hessian and it’s inverse is computationally very expensive(having computational complexity of \\(O(n^3)\\) for functions with n variables.\nIn order to use this method for optimization, the hessian needs to be positive definite which may not always be possible.\nHence to overcome these scenarios, Quasi-newton optimization algorithms can be used like the BFGS, and the LBFGS, where we try to approximate the hessian instead of calculating it."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#l-bfgs-for-optimizing-functions",
    "href": "posts/2ndorder/index(2) (1).html#l-bfgs-for-optimizing-functions",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "The BFGS algorithm constructs an approximation of the inverse Hessian matrix using a sequence of rank-two updates. This approximation captures information about the curvature of the objective function’s landscape and guides the optimization process. BFGS has good convergence properties and doesn’t require the explicit computation of the Hessian matrix, making it suitable for problems with a large number of variables.\nL-BFGS is a variant of the BFGS algorithm that addresses the memory and computational requirements associated with the Hessian matrix. In high-dimensional optimization problems, storing and manipulating the full Hessian matrix can be expensive. L-BFGS overcomes this limitation by maintaining a limited-memory approximation of the Hessian, using only a small number of vectors.\nL-BFGS uses a recursive formula to update and approximate the inverse Hessian matrix. Instead of storing the full Hessian matrix explicitly, L-BFGS maintains a limited number of vector pairs to approximate the Hessian. This makes L-BFGS well-suited for large-scale optimization problems and enables it to operate efficiently in high-dimensional spaces.\nThe following code implements LBFGS of the function -sin(x)-cos(y)\n\nimport torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    return -torch.sin(x[0])-torch.cos(x[1])\n\n# L-BFGS\ndef closure():\n    lbfgs.zero_grad()\n    objective = f(x_lbfgs)\n    objective.backward()\n    return objective\n\nx_lbfgs = torch.ones(2, 1)\nx_lbfgs.requires_grad = True\n\nlbfgs = optim.LBFGS([x_lbfgs],\n                    history_size=10, \n                    max_iter=4, \n                    line_search_fn=\"strong_wolfe\")\n                    \nhistory_lbfgs = []\nfor i in range(100):\n    history_lbfgs.append(f(x_lbfgs).item())\n    lbfgs.step(closure)\n\nLet us also perform gradient descent on this, with learning rate of 10^-5.\n\nx_gd = torch.ones(2, 1)\nx_gd.requires_grad = True\ngd = optim.SGD([x_gd], lr=1e-5)\n\nhistory_gd = []\nfor i in range(100):\n    gd.zero_grad()\n    objective = f(x_gd)\n    objective.backward()\n    gd.step()\n    history_gd.append(objective.item())\n\nNow, to visualize the results, we use a contour plot:\n\nx_range = np.linspace(-5, 5, 400)\ny_range = np.linspace(-5, 5, 400)\nX, Y = np.meshgrid(x_range, y_range)\n\nZ = f(torch.tensor([X, Y])).detach().numpy()\n\nfig=plt.figure(figsize=(10,6))\nplt.contourf(X, Y, Z, levels=20, cmap=\"viridis\")\n\ncoordinates = np.array([2.0, 1.0])\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"cyan\", label=\"Initial Coordinates\")\n\ncoordinates = x_lbfgs.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"red\", label=\"LBFGS\")\n\ncoordinates = x_gd.detach().numpy()\nplt.plot(coordinates[0], coordinates[1], marker=\"o\", color=\"orange\", label=\"Grad Desc (lr=1e-5)\")\n\nplt.colorbar(label=\"Objective Value\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Contour Plot of -sin(X)-cos(Y)\")\nplt.legend()\nplt.show()\n\n/var/folders/76/s25grzw958ld2_m4fhl5j5900000gn/T/ipykernel_49912/298132734.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n  Z = f(torch.tensor([X, Y])).detach().numpy()\n\n\n\n\n\nWe observe that in a 100 iterations, the gradient descent algorithm does not converge to the minima, but remains somewhere in between. Changing the learning rate might lead to the optimal value. For L-BFGS, the convergence is at the minima."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#remarks1",
    "href": "posts/2ndorder/index(2) (1).html#remarks1",
    "title": "Second order Optimization methods.",
    "section": "",
    "text": "The LBFGS method is appealing for several reasons it is very simple to implement it requires only function and gradient values and no other information on the problem # and it can be faster than the partitioned quasi Newton method on problems where the element functions depend on more than or variables\nIn addition the LBFGS method appears to be preferable to PQN for large problems in which the Hessian matrix is not very sparse or for problems in which the information on the separablity of the ob jective function is difficult to obtain."
  },
  {
    "objectID": "posts/2ndorder/index(2) (1).html#footnotes",
    "href": "posts/2ndorder/index(2) (1).html#footnotes",
    "title": "Second order Optimization methods.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiu, D.C. and Nocedal, J. (no date) On the limited memory BFGS method for large scale optimization - mathematical programming, SpringerLink. Available at: https://link.springer.com/article/10.1007/BF01589116 (Accessed: 20 August 2023).↩︎"
  },
  {
    "objectID": "posts/sirentask/siren.html",
    "href": "posts/sirentask/siren.html",
    "title": "Siren",
    "section": "",
    "text": "Implicit Neural representations using sinusoidal activation\nThe reason we use the sine function as an activation function is because it is infinitely differentiable where as for a function like relu, the double derivative becomes zero. Let’s try regenerating an audio file by mapping coordinates in the grid with amplitudes\nThe following is the ground truth audio\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWe are using a neural network with 3 hidden layers and each layer consisting of 256 neurons and we are using mean squared error loss function and optimizing it using Adam with a learning rate of 0.0001. We are training the neural network for a total of 1000 steps and displaying every 100th result. As can be seen in the following figures as the number of steps increases both the graphs converge.\nDescription of neural network\n\n\nSiren(\n  (net): Sequential(\n    (0): SineLayer(\n      (linear): Linear(in_features=1, out_features=256, bias=True)\n    )\n    (1): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (3): SineLayer(\n      (linear): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (4): Linear(in_features=256, out_features=1, bias=True)\n  )\n)\n\n\n\n\nStep 0, Total loss 0.025424\nStep 100, Total loss 0.002972\nStep 200, Total loss 0.001123\nStep 300, Total loss 0.000861\nStep 400, Total loss 0.000702\nStep 500, Total loss 0.000586\nStep 600, Total loss 0.000535\nStep 700, Total loss 0.000538\nStep 800, Total loss 0.000665\nStep 900, Total loss 0.000362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above graph the coordinates of the grid are squeezed to a single dimension (x-axis) and the corresponding model output is shown on the y-axis.\nFollowing is the audio regenerated using a neural network with sinusoidal activation\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThere is a little bit of noise in the above regenerated audio signal but overall has a good performance\nLet’s try using a different activation function such as relu and how well it can perform to regenerate audio and compare it with sinusoidal activation\n\n\nStep 0, Total loss 0.032153\nStep 100, Total loss 0.032153\nStep 200, Total loss 0.032153\nStep 300, Total loss 0.032153\nStep 400, Total loss 0.032153\nStep 500, Total loss 0.032153\nStep 600, Total loss 0.032153\nStep 700, Total loss 0.032153\nStep 800, Total loss 0.032153\nStep 900, Total loss 0.032153\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing is the audio regenerated through relu activation which is not at all close to the ground truth\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHow much memory do we save through this compression technique?\n\n\nMemory of audio file: 1232886 bytes\n\n\n\n\nNumber of parameters in the neural network are 198145\n\n\n\n\nMemory consumed by the neural network is 796343 bytes"
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html",
    "href": "posts/markov chain/index 2 (1).html",
    "title": "Markov Chains",
    "section": "",
    "text": "Markov models are mathematical models which are used to model sequential data, where the current observation is dependant on the past observations. Markov chains are the simplest markov models wherein, the current observation is only dependant on the previous observation and not dependant on observations prior to previous observations.\nThis can be represented by,\n\\[\nP(x_t|x_1,x_2,...x_{t-1})=P(x_t|x_{t-1})\n\\]\nFollowing is how we can represent the dependencies in data\n\n\n\n\n\nWe can can calculate probability of a sequence as follows,\n\\[\nP(x_1,x_2,..x_n)=P(x_1)P(x_2|x_1)P(x_3|x_2)...P(x_n|x_{n-1})\n\\]\nWhat this basically says is, The probability of any observation \\(x_i\\), is only dependant on \\(x_{i-1} \\quad \\forall \\quad i \\in [2,n]\\).\n\\(P(x_1)\\), is called the prior probability for the state/observation initially. The prior probability is the probability for starting from one of the states. It is denoted by \\(\\pi_i=P(x_1=i)\\), where \\(i\\) denotes the initial state from all possible states. The prior probability is one of the parameters of the Markov chain model.\nAnother parameter for the Markov chain model is the transition matrix denoted by \\(A\\). If there are \\(K\\) states, the transition matrix will be a \\(K\\)x\\(K\\) matrix, where \\(A_{ij}=P(x_t=j|x_{t-1}=i)\\).\nLet’s take an example for 3 states.\nWe can use the markov chain model to predict which city we will go to next, given the city we are in currently.\nLet’s assume the 3 cities/states to be Bangalore, Chennai and Mumbai and following is the transition matrix for this example.\n\n\n\n\nBangalore\nChennai\nMumbai\n\n\n\n\nBangalore\n0.3\n0.4\n0.3\n\n\nChennai\n0.2\n0.1\n0.7\n\n\nMumbai\n0.4\n0.4\n0.2\n\n\n\nThe sum of each row of the transition matrix must sum up to 1 as it covers all the possibilities.\nFollowing is the markov transition graph, which shows the probabilities of going from one city to another.\n\n\n\n\n\n\n\nWe can generate a sequence of observations using the following,\n\nSelect initial state\\((x_1)\\) using \\(\\pi\\).\nSample the state\\((x_t)\\) from \\(A\\) and \\(x_{t-1}\\), for \\(t \\in[2,..,T]\\).\n\nLet’s take the above example and generate a sequence of 6 time stamps.\n\n\n\n\n\n\n\n\n\n\n\n\nprior probability\n\n\n\n\nBangalore\n0.4\n\n\nChennai\n0.2\n\n\nMumbai\n0.4\n\n\n\n\n\n\n\nYou can change the values of prior probabilities and probabilities in the transition matrix in the link given below.\nhttps://vannshmarkovchain.streamlit.app/\n\n\n\nLet’s understand some properties of markov chains with the following example\n\n\n\n\n\nWe know that if there is an arrow from state A to state B, then there is a non-zero transition probability from state A to state B. If we start random walk from state “0” in the above diagram, we can never come back to state zero even after infinite steps. Such a state where we cannot come back to is called a transient state. Hence, in the above example “0” is a transient state. If we look at state “1” or state “2” we know that we are bound to come back to the same state after some steps. Such a state is called a recurrent state. Here, state “1” and “2” are examples of recurrent states.\nIn a markov chain, if all states not reachable from all other states, we say that the markov chain is reducible. In the above example, we cannot reach state “0” from state “1” or “2”.\nIf we add an arrow from state “2” to state “0”, then it is possible to come back to state “0” and hence, “0” is not a transient state any more.\n\n\n\n\n\nSuch a markov chain, where it is possible to go from every state to another(not necessarily in one move) is called an ergodic markov chain.\n\n\n\nLet’s consider the cities example again, suppose we wanted to calculate the probability of travelling from Bangalore to Chennai in 2 steps, we will have to consider all possible cases.\n\\[\nA=\\begin{bmatrix} 0.3 & 0.4 & 0.3 \\\\ 0.2 & 0.1 & 0.7 \\\\ 0.4 & 0.4 & 0.2 \\end{bmatrix}\n\\]\nLet’s denote probability of going from state i to state j in n steps as \\(p_{ij}(n)\\). In this example 0 denotes Bangalore, 1 denotes Chennai and 2 denotes Mumbai.\n\\[\np_{01}(2) = p_{02}(1)p_{21}(1)+p_{00}(1)p_{01}(1)+p_{01}(1)p_{11}(1)\n\\]\nNote that \\(p_{02}(1)\\) is just equal to \\(A_{02}\\). Hence, the above equation can be written as\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= A_{02}A_{21}+A_{00}A_{01}+A_{01}A_{11} \\\\ p_{01}(2) &= \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\end{bmatrix} \\begin{bmatrix} A_{01} \\\\ A_{11} \\\\ A_{21} \\end{bmatrix} \\end{split} \\end{equation}\n\\]\nHence \\(p_{01}(2)\\) is the dot product of the 0th row and 1st column of matrix A. Hence \\(p_{01}(2)\\) is the element present in the 0th row and the 1st column in \\(AXA\\) or \\(A^2\\). Let’s verify it.\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= 0.3*0.4+0.3*0.4+0.4*0.1 \\\\ p_{01}(2) &= 0.28 \\\\ A^2 &= \\begin{bmatrix} 0.29 & 0.28 & 0.43 \\\\ 0.36 & 0.37 & 0.27\\\\ 0.28 & 0.28 & 0.44 \\end{bmatrix}\\end{split}\\end{equation}\n\\]\nHence, \\(A^n_{ij}\\) denotes the probability of moving from state i to state j in n steps. If some power of the trasnition matrix A has all positive values then it is possible to move from every state to every other where the number of steps is equal to the power taken of A and hence, A is ergodic."
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#markov-chain-sampling",
    "href": "posts/markov chain/index 2 (1).html#markov-chain-sampling",
    "title": "Markov Chains",
    "section": "",
    "text": "We can generate a sequence of observations using the following,\n\nSelect initial state\\((x_1)\\) using \\(\\pi\\).\nSample the state\\((x_t)\\) from \\(A\\) and \\(x_{t-1}\\), for \\(t \\in[2,..,T]\\).\n\nLet’s take the above example and generate a sequence of 6 time stamps.\n\n\n\n\n\n\n\n\n\n\n\n\nprior probability\n\n\n\n\nBangalore\n0.4\n\n\nChennai\n0.2\n\n\nMumbai\n0.4\n\n\n\n\n\n\n\nYou can change the values of prior probabilities and probabilities in the transition matrix in the link given below.\nhttps://vannshmarkovchain.streamlit.app/"
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#markov-chain-properties",
    "href": "posts/markov chain/index 2 (1).html#markov-chain-properties",
    "title": "Markov Chains",
    "section": "",
    "text": "Let’s understand some properties of markov chains with the following example\n\n\n\n\n\nWe know that if there is an arrow from state A to state B, then there is a non-zero transition probability from state A to state B. If we start random walk from state “0” in the above diagram, we can never come back to state zero even after infinite steps. Such a state where we cannot come back to is called a transient state. Hence, in the above example “0” is a transient state. If we look at state “1” or state “2” we know that we are bound to come back to the same state after some steps. Such a state is called a recurrent state. Here, state “1” and “2” are examples of recurrent states.\nIn a markov chain, if all states not reachable from all other states, we say that the markov chain is reducible. In the above example, we cannot reach state “0” from state “1” or “2”.\nIf we add an arrow from state “2” to state “0”, then it is possible to come back to state “0” and hence, “0” is not a transient state any more.\n\n\n\n\n\nSuch a markov chain, where it is possible to go from every state to another(not necessarily in one move) is called an ergodic markov chain."
  },
  {
    "objectID": "posts/markov chain/index 2 (1).html#transition-in-n-steps",
    "href": "posts/markov chain/index 2 (1).html#transition-in-n-steps",
    "title": "Markov Chains",
    "section": "",
    "text": "Let’s consider the cities example again, suppose we wanted to calculate the probability of travelling from Bangalore to Chennai in 2 steps, we will have to consider all possible cases.\n\\[\nA=\\begin{bmatrix} 0.3 & 0.4 & 0.3 \\\\ 0.2 & 0.1 & 0.7 \\\\ 0.4 & 0.4 & 0.2 \\end{bmatrix}\n\\]\nLet’s denote probability of going from state i to state j in n steps as \\(p_{ij}(n)\\). In this example 0 denotes Bangalore, 1 denotes Chennai and 2 denotes Mumbai.\n\\[\np_{01}(2) = p_{02}(1)p_{21}(1)+p_{00}(1)p_{01}(1)+p_{01}(1)p_{11}(1)\n\\]\nNote that \\(p_{02}(1)\\) is just equal to \\(A_{02}\\). Hence, the above equation can be written as\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= A_{02}A_{21}+A_{00}A_{01}+A_{01}A_{11} \\\\ p_{01}(2) &= \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\end{bmatrix} \\begin{bmatrix} A_{01} \\\\ A_{11} \\\\ A_{21} \\end{bmatrix} \\end{split} \\end{equation}\n\\]\nHence \\(p_{01}(2)\\) is the dot product of the 0th row and 1st column of matrix A. Hence \\(p_{01}(2)\\) is the element present in the 0th row and the 1st column in \\(AXA\\) or \\(A^2\\). Let’s verify it.\n\\[\n\\begin{equation}\\begin{split} p_{01}(2) &= 0.3*0.4+0.3*0.4+0.4*0.1 \\\\ p_{01}(2) &= 0.28 \\\\ A^2 &= \\begin{bmatrix} 0.29 & 0.28 & 0.43 \\\\ 0.36 & 0.37 & 0.27\\\\ 0.28 & 0.28 & 0.44 \\end{bmatrix}\\end{split}\\end{equation}\n\\]\nHence, \\(A^n_{ij}\\) denotes the probability of moving from state i to state j in n steps. If some power of the trasnition matrix A has all positive values then it is possible to move from every state to every other where the number of steps is equal to the power taken of A and hence, A is ergodic."
  },
  {
    "objectID": "posts/matrix low rank/index (3).html",
    "href": "posts/matrix low rank/index (3).html",
    "title": "Matrix transformations and Low-rank matrices",
    "section": "",
    "text": "Matrix Transformations\nMatrix transformation or linear transformation of a vector v is pre-multiplying the vector by a matrix (called the transformation matrix) which results in v being transformed.\nLet us first interpret matrix transformations in the 2D space.\n\nfrom sympy import Matrix, MatrixSymbol, Eq, MatMul\n\nsympy_v = MatrixSymbol(\"v\",2,1) # Column vector having 2 rows\nsympy_A = MatrixSymbol(\"A\",2,2) # 2x2 Transformation matrix\nsympy_y = MatrixSymbol(\"y\", 2, 1)\nprint(\"y is the transformed vector.\")\nEq(sympy_y,sympy_A*sympy_v,evaluate=False)\n\ny is the transformed vector.\n\n\n\\(\\displaystyle y = A v\\)\n\n\n\nExample\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nA = np.array([[3,3],[2,6]])\nv = np.array([1,1])\nAv = A @ v\nplt.arrow(0,0,1,1,head_width=0.2,width=0.05,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.2,width=0.05,color='red',label=\"Av\")\nplt.legend([f\"v = {v}\",f\"Av = {Av}\"],loc='lower right')\nplt.title(f\"A = {A}\")\nplt.show()\n\n\n\n\n\nEq(Matrix(Av),MatMul(Matrix(A),Matrix(v)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}6\\\\8\\end{matrix}\\right] = \\left[\\begin{matrix}3 & 3\\\\2 & 6\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\nIn the above equation,\nA = \\(\\begin{bmatrix} 3&3\\\\2&6\\end{bmatrix}\\), v = \\(\\begin{bmatrix} 1\\\\1 \\end{bmatrix}\\) and y = \\(\\begin{bmatrix} 6\\\\8 \\end{bmatrix}\\)\n\n\n\n\nNote:\n\n\nIf the transformation matrix A is taken to be \\(\\begin{bmatrix} \\cos(\\theta)&-\\sin(\\theta)\\\\ \\sin(\\theta)&\\cos(\\theta) \\end{bmatrix}\\)\n\n\nthen the vector v is rotated in an anti-clockwise direction by an angle \\(\\theta\\).\nFor example, Let’s take v = \\(\\begin{bmatrix} 1\\\\0 \\end{bmatrix}\\) and \\(\\theta\\)=90 deg, therefore A = \\(\\begin{bmatrix} 0&-1\\\\1&0 \\end{bmatrix}\\)\n\nA = np.array([[0,-1],[1,0]])\nv = np.array([1,0])\nAv = A @ v\nplt.figure()\nplt.arrow(0,0,1,0,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.legend([f\"v = {v}\",f\"Av = {Av}\"])\nplt.title(f\"A = {A}\")\nplt.show()\n\n\n\n\n\nEq(Matrix(Av),MatMul(Matrix(A),Matrix(v)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}0\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\0\\end{matrix}\\right]\\)\n\n\nAs a result of the transformation, vector v is rotated by 90 degrees in the counter-clockwise direction.\n\n\n\n\nUnderstanding Matrix transformations of low-rank matrices\nLow-rank matrices are matrices which have rank less than the dimensionality of the column space of the matrix. For eg. a 3x3 transformation matrix will be a low rank matrix if it’s column space can be spanned by less than or equal to 2 vectors. In other words, the column vectors are linearly dependant.\nExample of a low-rank matrix is \\(\\begin{bmatrix} 1&0&1\\\\0.5&1&1.5\\\\1&0&1 \\end{bmatrix}\\)\nThe determinant of the transformation matrix having low-rank is zero.\nGeometrically what this means is transforming the vector using a low-rank matrix results in the subspace of the original vector space. Hence for example if the original vector space is \\(R^3\\), the resultant subspace can be \\(R^2\\), \\(R\\), or in an extreme case even a point.\n\nExample\n\nA = np.array([[1,-1],[3,-3]]) # Low-rank 2x2 matrix\n# Example 1\nv = np.array([2,3])\nAv = A @ v\nplt.figure()\nplt.subplot(1,2,1)\nplt.arrow(0,0,2,3,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.title(f\"A = {A}\")\nx = np.linspace(-3,3)\ny = x*3\nplt.plot(x,y,color=\"black\",alpha=0.3)\nplt.legend([f\"v = {v}\",f\"Av = {Av}\",\"y=3x\"])\n\n# Example 2\nv1 = np.array([-1.7,2.2])\nplt.subplot(1,2,2)\nplt.arrow(0,0,-1.7,2.2,head_width=0.1,width=0.01,color=\"blue\",label=\"v\")\nplt.arrow(0,0,Av[0],Av[1],head_width=0.1,width=0.01,color='red',label=\"Av\")\nplt.title(f\"A = {A}\")\nx = np.linspace(-3,3)\ny = x*3\nplt.plot(x,y,color=\"black\",alpha=0.3)\nplt.legend([f\"v = {v}\",f\"Av = {Av}\",\"y=3x\"])\nplt.show()\n\n\n\n\nIn the above plots, Av (transformed vector) lies on the line \\(y=3x\\), and does not depend on the coordinates of vector v. As a result, using a low-rank matrix to transform a vector results only in the subspace of the original vector space. Here the subspace is the line \\(y=3x\\) and the original vector space is \\(R^2\\)."
  },
  {
    "objectID": "posts/Log-likelihood/index 2.html",
    "href": "posts/Log-likelihood/index 2.html",
    "title": "Log-likelihood of normal distribution",
    "section": "",
    "text": "Generating samples from a univariate normal distribution of mean and variance equal to 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmean = 1\nvariance = 1\nnum_samples = 1000\nnp.random.seed(1)\nsamples = np.random.normal(loc=mean, scale=np.sqrt(variance), size=num_samples)\nprint(samples.shape)\n\n(1000,)\n\n\nLet’s analyze the samples\n\n# First 5 elements\nprint(samples[:5])\n\n[ 2.62434536  0.38824359  0.47182825 -0.07296862  1.86540763]\n\n\nVisualizing the random samples\n\nplt.hist(samples,bins=30)\nplt.title(\"Histogram of samples with mean and variance 1\")\nplt.show()\n\n\n\n\nThe likelihood for the normal distribution is given by,\n\\[\nL(\\mu,\\sigma^2;x_1,x_2,....,x_n) = \\prod_{j=1}^n f(x_j;\\mu,\\sigma^2)\n\\]\nwhere f, is the probability density function which in this case is the univariate normal distribution given by,\n\\[\nf(x,\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^\\frac{-(x-\\mu)^2}{2\\sigma^2}\n\\]\nThe likelihood of a distribution with given mean and variance is the product of value of PDF at all sample points with given mean and variance. Likelihood gives us a fair estimate or probablity as to how well the distribution is fitting the data for given parameters (mean and variance in case of normal distributon). So the probability that the normal distribution with given mean and variance will fit the samples is a product of the value of the pdf at each sample point as all these events are independant events. Hence, plotting the likelihood or the log-likelihood as function of parameters (in this case mean and variance) can give us an estimate as to which values of parameters are needed to best fit the data.\nLikelihoods are generally products of many numbers and products are not numerically stable. Hence, log-likelihood is calculated to make computations less expensive and simpler.\nTaking log on both sides,\n\\[\n\\begin{equation} \\begin{split}\nl(\\mu,\\sigma^2;x_1,x_2,...,x_n) & = ln(L(\\mu,\\sigma^2;x_1,x_2,...,x_n)) \\\\\n& = ln((2\\pi\\sigma^2)^\\frac{-n}{2} e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2})\\\\ & = ln(2\\pi\\sigma^2)^\\frac{-n}{2}+ln(e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2}))\\\\ &= \\frac{-n}{2}ln(2\\pi\\sigma^2)-\\Sigma_{j=1}^n \\frac{(x_j-\\mu)^2}{2\\sigma^2}  \\end{split} \\end{equation}\n\\]\nThe above equation can be used to compute the log-likelihood.\n\nlog_likelihoods = []\nfor mean in np.linspace(0, 2, 100):\n    for var in np.linspace(0.1, 2, 100):\n        log_likelihood = np.sum(-0.5 * np.log(2 * np.pi * var) - ((samples - mean) ** 2) / (2 * var))\n        log_likelihoods.append((mean, var, log_likelihood))\n\nlog_likelihoods = np.array(log_likelihoods)\nprint(log_likelihoods.shape)\n\n(10000, 3)\n\n\nThe log_likelihoods array is a 2D-array with mean values in the first column, variance values in the second column and likelihood values in the third column. Let’s look at the first 5 rows of the array.\n\nlog_likelihoods[:5]\n\narray([[ 0.00000000e+00,  1.00000000e-01, -9.97514834e+03],\n       [ 0.00000000e+00,  1.19191919e-01, -8.41934984e+03],\n       [ 0.00000000e+00,  1.38383838e-01, -7.30630086e+03],\n       [ 0.00000000e+00,  1.57575758e-01, -6.47285210e+03],\n       [ 0.00000000e+00,  1.76767677e-01, -5.82700895e+03]])\n\n\nA 3D plot of how log-likelihood varies with mean and variance can be seen below.\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.plot3D(log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2],color='green')\nax.set_title(\"Log-likelihood of normal distribution as a function of mean and variance\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nax.set_zlabel(\"Log-likelihood\")\nplt.show()\n\n\n\n\nWe can also visualize 3D plots in 2D using a contour plot. To do so, we can create a 100x100 grid (because we have 100 mean and variance values) and reshape Z’s shape to 100x100 so we have a value of log-likelihood for each point on the grid.\n\n# Contour plot\nfig,ax = plt.subplots()\nX,Y,Z = log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2]\nX,Y = np.meshgrid(np.unique(X),np.unique(Y))\nZ = Z.reshape(X.shape)\nplt.contourf(X,Y,Z)\nplt.colorbar()\nax.set_title(\"Contour plot\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nplt.show()\n\n\n\n\nThe values of mean and variance which will best fit the data will be those having the highest value of log-likelihood (Z-axis) and hence in the “yellow” region of the contour plot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Siren\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nShannon Fano encoding\n\n\n\n\n\nWhat is shannon fano encoding and how is it used in data compression.\n\n\n\n\n\n\nAug 30, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMarkov Chains\n\n\n\n\n\nUnderstanding Markov chains.\n\n\n\n\n\n\nAug 27, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Decompositions\n\n\n\n\n\nUnderstanding Eigenvalue decompositions and SVD.\n\n\n\n\n\n\nAug 23, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nSecond order Optimization methods.\n\n\n\n\n\nUnderstanding optimization of functions using Newton’s method and L-BFGS.\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTaylor series\n\n\n\n\n\nUnderstanding and visualizing 1D and 2D taylor series.\n\n\n\n\n\n\nAug 14, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nLog-likelihood of normal distribution\n\n\n\n\n\nPlotting and understanding log-likelihood as a function of parameters for normal distribution.\n\n\n\n\n\n\nAug 1, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\n  \n\n\n\n\nMatrix transformations and Low-rank matrices\n\n\n\n\n\nUnderstanding matrix transformations as pre-multiplying a vector with a matrix and interpreting matrix transformations using low-rank matrices.\n\n\n\n\n\n\nJul 31, 2023\n\n\nVannsh Jani\n\n\n\n\n\n\nNo matching items"
  }
]