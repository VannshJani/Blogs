























import numpy as np

def joint_prob(val_x,val_y,X,Y):
    n = len(X)
    count=0
    for i in range(n):
        if (X[i]==val_x and Y[i]==val_y):
            count+=1
    return count/n


def mutual_info(X,Y):
    n = len(X)
    unique_x = list(set(X))
    unique_y = list(set(Y))
    prob_x = [X.count(x)/n for x in unique_x]
    prob_y = [Y.count(y)/n for y in unique_y]
    MI=0
    for i in range(len(unique_x)):
        for j in range(len(unique_y)):
            px = prob_x[i]
            py = prob_y[j]
            jointp = joint_prob(unique_x[i],unique_y[j],X,Y)
            if jointp==0:
                continue
            else:
                MI += jointp * np.log(jointp/(px*py))

    return MI


X = ["A","B","C","A","B","C"]
Y = ["A","B","C","A","B","C"]
Z = ["A","A","A","A","A","A"]
W = ["A","A","C","A","A","C"]
print(f"Mutual information of X and Y is {mutual_info(X,Y)}") 
print(f"Mutual information of X and W is {mutual_info(X,W)}")
print(f"Mutual information of X and Z is {mutual_info(X,Z)}") 





from sklearn.metrics import mutual_info_score as MIS
print(f"Mutual information of X and Y through sklearn is {MIS(X,Y)}") 
print(f"Mutual information of X and W through sklearn is {MIS(X,W)}") 
print(f"Mutual information of X and Z through sklearn is {MIS(X,Z)}") 





























def entropy(X,bins):

    binned = np.histogram(X,bins)[0]
    prob = binned / np.sum(binned)
    prob = prob[np.nonzero(prob)]
    entropy = - np.sum(prob* np.log2(prob))
    return(entropy)




def joint_entropy(X,Y,bins):
    
    binned_dist = np.histogram2d(X,Y,bins)[0]
    probs = binned_dist / np.sum(binned_dist)
    probs = probs[np.nonzero(probs)]
    joint_entropy = - np.sum(probs* np.log2(probs))
    return(joint_entropy)




def mutual_info(X,Y,bins):

    H_X = entropy(X,bins)
    H_Y = entropy(Y,bins)
    H_XY = joint_entropy(X,Y,bins)
    MI = H_X + H_Y - H_XY
    return(MI)

np.random.seed(12)
X = np.random.normal(0,1,30)
Y = np.zeros(30)
Z = X*2
W = np.linspace(1,10,30)

print("Taking number of bins as 10")
print("Mutual information of X and Y is: ",mutual_info(X,Y,10))
print("Mutual information of X and Z is: ",mutual_info(X,Z,10))
print("Mutual information of X and W is: ",mutual_info(X,W,10))

















import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
noise = np.random.normal(0,1,30)

alpha = int(input("alpha: "))
beta  = int(input("beta : "))

x = np.random.uniform(1,10,30)
y = np.random.uniform(1,10,30)
X = alpha*(np.log(x))
Y = beta*(y**2 )
W = X+Y+noise 

plt.scatter(x, X, label='alog(x)')
plt.title("Dependencies of Target with features individually")
plt.xlabel("Feature-1")
plt.ylabel("Target")
plt.show()

plt.scatter(y, Y, label='by^0.5', color='orange')
plt.xlabel("Feature-2")
plt.ylabel("Target")
plt.show()



from sklearn.feature_selection import mutual_info_regression as MIR
print(f"MI score of W and x is {MIR(X.reshape(-1,1),W)[0]}")
print(f"MI score of W and y is {MIR(Y.reshape(-1,1),W)[0]}")



