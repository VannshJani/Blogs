{
  "hash": "656d3e5e0b123d114f1fe97d021266bf",
  "result": {
    "markdown": "---\ntitle: \"Markov Chains\"\nexecute: \n  echo: false\ndescription: \"Understanding Markov chains.\"\nauthor: \"Vannsh Jani\"\ndate: \"08/27/2023\"\ndraft: false\n---\n\n# What are Markov models?\n\nMarkov models are mathematical models which are used to model sequential data, where the current observation is dependant on the past observations. Markov chains are the simplest markov models wherein, the current observation is only dependant on the previous observation and not dependant on observations prior to previous observations.\n\nThis can be represented by,\n\n$$\nP(x_t|x_1,x_2,...x_{t-1})=P(x_t|x_{t-1})\n$$\n\nFollowing is how we can represent the dependencies in data\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index 2 (1)_files/figure-html/cell-2-output-1.svg){}\n:::\n:::\n\n\nWe can can calculate probability of a sequence as follows,\n\n$$\nP(x_1,x_2,..x_n)=P(x_1)P(x_2|x_1)P(x_3|x_2)...P(x_n|x_{n-1})\n$$\n\nWhat this basically says is, The probability of any observation $x_i$, is only dependant on $x_{i-1} \\quad \\forall \\quad i \\in [2,n]$.\n\n$P(x_1)$, is called the prior probability for the state/observation initially. The prior probability is the probability for starting from one of the states. It is denoted by $\\pi_i=P(x_1=i)$, where $i$ denotes the initial state from all possible states. The prior probability is one of the parameters of the Markov chain model.\n\nAnother parameter for the Markov chain model is the transition matrix denoted by $A$. If there are $K$ states, the transition matrix will be a $K$x$K$ matrix, where $A_{ij}=P(x_t=j|x_{t-1}=i)$.\n\nLet's take an example for 3 states.\n\nWe can use the markov chain model to predict which city we will go to next, given the city we are in currently.\n\nLet's assume the 3 cities/states to be Bangalore, Chennai and Mumbai and following is the transition matrix for this example.\n\n|               | Bangalore | Chennai | Mumbai |\n|:-------------:|:---------:|:-------:|:------:|\n| **Bangalore** |    0.3    |   0.4   |  0.3   |\n|  **Chennai**  |    0.2    |   0.1   |  0.7   |\n|  **Mumbai**   |    0.4    |   0.4   |  0.2   |\n\nThe sum of each row of the transition matrix must sum up to 1 as it covers all the possibilities.\n\nFollowing is the markov transition graph, which shows the probabilities of going from one city to another.\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](index 2 (1)_files/figure-html/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Markov chain sampling\n\nWe can generate a sequence of observations using the following,\n\n1.  Select initial state$(x_1)$ using $\\pi$.\n2.  Sample the state$(x_t)$ from $A$ and $x_{t-1}$, for $t \\in[2,..,T]$.\n\nLet's take the above example and generate a sequence of 6 time stamps.\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](index 2 (1)_files/figure-html/cell-4-output-1.svg){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prior probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Bangalore</th>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>Chennai</th>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>Mumbai</th>\n      <td>0.4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nYou can change the values of prior probabilities and probabilities in the transition matrix in the link given below.\n\n<https://vannshmarkovchain.streamlit.app/>\n\n## Markov chain properties\n\nLet's understand some properties of markov chains with the following example\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](index 2 (1)_files/figure-html/cell-5-output-1.svg){}\n:::\n:::\n\n\nWe know that if there is an arrow from state A to state B, then there is a non-zero transition probability from state A to state B. If we start random walk from state \"0\" in the above diagram, we can never come back to state zero even after infinite steps. Such a state where we cannot come back to is called a **transient state.** Hence, in the above example \"0\" is a transient state. If we look at state \"1\" or state \"2\" we know that we are bound to come back to the same state after some steps. Such a state is called a **recurrent state.** Here, state \"1\" and \"2\" are examples of recurrent states.\n\nIn a markov chain, if all states not reachable from all other states, we say that the markov chain is **reducible.** In the above example, we cannot reach state \"0\" from state \"1\" or \"2\".\n\nIf we add an arrow from state \"2\" to state \"0\", then it is possible to come back to state \"0\" and hence, \"0\" is not a transient state any more.\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](index 2 (1)_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\nSuch a markov chain, where it is possible to go from every state to another(not necessarily in one move) is called an **ergodic markov chain.**\n\n## Transition in n steps\n\nLet's consider the cities example again, suppose we wanted to calculate the probability of travelling from Bangalore to Chennai in 2 steps, we will have to consider all possible cases.\n\n$$\nA=\\begin{bmatrix} 0.3 & 0.4 & 0.3 \\\\ 0.2 & 0.1 & 0.7 \\\\ 0.4 & 0.4 & 0.2 \\end{bmatrix}\n$$\n\nLet's denote probability of going from state i to state j in n steps as $p_{ij}(n)$. In this example 0 denotes Bangalore, 1 denotes Chennai and 2 denotes Mumbai.\n\n$$\np_{01}(2) = p_{02}(1)p_{21}(1)+p_{00}(1)p_{01}(1)+p_{01}(1)p_{11}(1)\n$$\n\nNote that $p_{02}(1)$ is just equal to $A_{02}$. Hence, the above equation can be written as\n\n$$\n\\begin{equation}\\begin{split} p_{01}(2) &= A_{02}A_{21}+A_{00}A_{01}+A_{01}A_{11} \\\\ p_{01}(2) &= \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\end{bmatrix} \\begin{bmatrix} A_{01} \\\\ A_{11} \\\\ A_{21} \\end{bmatrix} \\end{split} \\end{equation}\n$$\n\nHence $p_{01}(2)$ is the dot product of the 0th row and 1st column of matrix A. Hence $p_{01}(2)$ is the element present in the 0th row and the 1st column in $AXA$ or $A^2$. Let's verify it.\n\n$$\n\\begin{equation}\\begin{split} p_{01}(2) &= 0.3*0.4+0.3*0.4+0.4*0.1 \\\\ p_{01}(2) &= 0.28 \\\\ A^2 &= \\begin{bmatrix} 0.29 & 0.28 & 0.43 \\\\ 0.36 & 0.37 & 0.27\\\\ 0.28 & 0.28 & 0.44 \\end{bmatrix}\\end{split}\\end{equation}\n$$\n\nHence, $A^n_{ij}$ denotes the probability of moving from state i to state j in n steps. If some power of the trasnition matrix A has all positive values then it is possible to move from every state to every other where the number of steps is equal to the power taken of A and hence, A is ergodic.\n\n",
    "supporting": [
      "index 2 (1)_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}