{
  "hash": "2cc847b54196990676650f7e6ee081ae",
  "result": {
    "markdown": "---\ntitle: \"Shannon Fano encoding\"\ndescription: \"What is shannon fano encoding and how is it used in data compression.\"\nauthor: \"Vannsh Jani\"\ndate: \"08/30/2023\"\ndraft: false\n---\n\n# Shannon fano encoding\n\nThe Shannon fano algorithm is a lossless data compression technique. Data compression is the process of encoding or converting data in such a way that it consumes less memory space. Hence, it reduces the resources required to store and transmit data.\n\nFor eg, it would make more sense to assign symbols or characters occuring more frequently to consume less number of bits than to assign equal bits to all characters. Hence, it assigns codes of variable lengths to each character. In the realm of data compression, two distinct but connected methods for creating a prefix code for the characters is Shannon Fano encoding.\n\n# Algorithm\n\n1.  We find out the frequency or probability of occurance of each character or symbol and store it in an array.\n2.  We sort the array in decreasing order based on probability(frequency).\n3.  Then, we split the list in two parts with as much similarity(least variance) between the frequency/probability of left and right parts.\n4.  We assign the binary digit '0' to the left part and binary digit '1' to the right part. This implies that symbols in the left section will begin with '0' and symbols in the right section will begin with '1'.\n5.  We repeat steps 3 and 4 by splitting each section into left and right sections and assigning the binary digits '0' and '1' to them unitl each section is left with just one symbol(analogous to leaf node of binary tree).\n\nLet's better understand shannon-fano and visualize it using examples. We'll encode the characters of the following sentence.\n\n**\"In the tranquil stillness of the early morning, birdsong filled the air as the first rays of sunlight painted the horizon with hues of pink and orange, creating a breathtakingly beautiful scene that seemed to belong to a world untouched by time.\"**\n\nLet's first calculate the probability of each character in the above statement, and sort them based on probabilities in descending order.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nsentence = \"aaaaaaaabbbbccd\"\ndef generate_prob(sentence):\n    d={}\n    special=0\n    for c in sentence:\n        if c==\",\" or c==\".\" or c==\" \":\n            special+=1\n            continue\n        elif c.lower() in d:\n            d[c.lower()] += 1\n        else:\n            d[c.lower()] = 1\n    total_char = len(sentence)-special\n    for key in d:\n        d[key] = d[key]/total_char\n    return d\nseq = generate_prob(sentence)\nseq=sorted(seq.items(),key=lambda item: item[1], reverse=True)\nd = dict(seq)\nd\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n{'a': 0.5333333333333333,\n 'b': 0.26666666666666666,\n 'c': 0.13333333333333333,\n 'd': 0.06666666666666667}\n```\n:::\n:::\n\n\nWe create nodes for the sequence and build a tree following the top to bottom approach.\n\nSince we want the splits to have similar total probabilities, we can assign '0' and '1' alternatively on the descended probability sequence or we can create the left subset until the probabilities of all the subsets approximately reach half of the total probability value.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass Node:\n    def __init__(self,char,prob):\n        self.char=char\n        self.p=prob\n        self.code=\"\"\n        self.left=None\n        self.right=None\n\ninitial=\"\"\nfor c in seq:\n    initial+=c[0]\nroot = Node(initial,1)\n\n\ndef split(node):\n    left_subset={}\n    right_subset={}\n    total_p = node.p\n    half_p = total_p/2\n    sum=0\n    index = 0\n    seq=node.char\n    for c in seq:\n        if sum>=half_p:\n            break\n        sum+=d[c]\n        index+=1\n    left_char = seq[:index]\n    right_char = seq[index:]\n    for l in left_char:\n        left_subset[l]=d[l]\n    for r in right_char:\n        right_subset[r]=d[r]\n    return left_subset,right_subset\n\ndef Build_tree(root):\n    if len(root.char)==1:\n        return\n    left_subset,right_subset = split(root)\n    left_char=\"\"\n    right_char=\"\"\n\n    for c in left_subset:\n        left_char+=c\n    for cr in right_subset:\n        right_char+=cr\n    left_prob = sum(left_subset.values())\n    right_prob = sum(right_subset.values())\n\n    left_node = Node(char=left_char,prob=left_prob)\n    right_node = Node(char=right_char,prob=right_prob)\n    root.left = left_node\n    root.right = right_node\n    Build_tree(left_node)\n    Build_tree(right_node)\n    \nBuild_tree(root)\n\ndef assign_codes(root):\n    if root.left is None and root.right is None:\n        return\n    if root.left is not None:\n        root.left.code = \"0\" + root.code\n    if root.right is not None:\n        root.right.code = \"1\" + root.code\n    assign_codes(root.left)\n    assign_codes(root.right)\nassign_codes(root)\n\ntotal_bits=0\ndef print_codes(root):\n    global total_bits\n    if root is not None:\n        if (len(root.char)==1):\n          total_bits += len(root.code)*sentence.count(root.char)\n          print(f\"Character is '{root.char}', probability is {round(root.p,4)} and code is {root.code}\")\n        print_codes(root.left)\n        print_codes(root.right)\nprint_codes(root)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCharacter is 'a', probability is 0.5333 and code is 0\nCharacter is 'b', probability is 0.2667 and code is 01\nCharacter is 'c', probability is 0.1333 and code is 011\nCharacter is 'd', probability is 0.0667 and code is 111\n```\n:::\n:::\n\n\nHence, we follow the top-bottom approach, where we add '0' to the left subset and '1' to the right subset as we traverse down the tree.\n\n# Visualizing the Binary Tree\n\n![](images/sf-01.png){fig-align=\"center\" width=\"278\" height=\"319\"}\n\n# Results and Observations\n\nIf we consider the \"ascii\" encoding, we would have to consider 8 bits for each letter, whereas we consume less number of bits using shannon-fano encoding.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(f\"No.of bits using ascii = {len(sentence)*8}\")\nprint(f\"No.of bits using shannon-fano = {total_bits}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo.of bits using ascii = 120\nNo.of bits using shannon-fano = 25\n```\n:::\n:::\n\n\nShannon-fano algorithm certainly performs better than the ascii encoding and does a decent job.\n\n### Compression ratio\n\nThe compression ratio tells us how much smaller the compressed data is compared to the original data. The higher the compression ratio, the better the compression.\n\nCompression ratio = Before Compression / After compression\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nb = len(sentence)*8\na = total_bits\nprint(f\"compression ratio is,\",(b/a))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncompression ratio is, 4.8\n```\n:::\n:::\n\n\n### Entropy\n\nEntropy is the measure of randomness of data, higher the entropy more random the data. More random the data, the more number of bits we need to measure it.\n\n$$\nH(x) = -\\Sigma_{i=1}^n p(x_i)log_2(p(x_i))\n$$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\ndef calc_entropy(p):\n  return -1 * p * np.log2(p)\n\nentropy = 0\nfor c in d:\n  entropy += calc_entropy(d[c])\n  \nprint(f\"Entropy of input data is {entropy}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEntropy of input data is 1.640223928941852\n```\n:::\n:::\n\n\nIt turns out, shannon fano algorithm is not the most optimized algorithm and may not work always. The Huffman encoding which is a slightly modified version of the shannon-fano optimizes the data compression even further.\n\n<https://vannshsf.streamlit.app/>\n\n",
    "supporting": [
      "index 2 (2)_files"
    ],
    "filters": [],
    "includes": {}
  }
}