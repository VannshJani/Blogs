{
  "hash": "c8e1939607c93ca991bc1024a9875a6c",
  "result": {
    "markdown": "---\ntitle: \"Log-likelihood of normal distribution\"\ndescription: \"Plotting and understanding log-likelihood as a function of parameters for normal distribution.\"\nauthor: \"Vannsh Jani\"\ndate: \"08/1/2023\"\ndraft: false\n---\n\n# Generating samples from a univariate normal distribution of mean and variance equal to 1.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmean = 1\nvariance = 1\nnum_samples = 1000\nnp.random.seed(1)\nsamples = np.random.normal(loc=mean, scale=np.sqrt(variance), size=num_samples)\nprint(samples.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1000,)\n```\n:::\n:::\n\n\nLet's analyze the samples\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# First 5 elements\nprint(samples[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 2.62434536  0.38824359  0.47182825 -0.07296862  1.86540763]\n```\n:::\n:::\n\n\nVisualizing the random samples\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.hist(samples,bins=30)\nplt.title(\"Histogram of samples with mean and variance 1\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index 2_files/figure-html/cell-4-output-1.png){width=575 height=431}\n:::\n:::\n\n\nThe likelihood for the normal distribution is given by,\n\n$$\nL(\\mu,\\sigma^2;x_1,x_2,....,x_n) = \\prod_{j=1}^n f(x_j;\\mu,\\sigma^2)\n$$\n\nwhere f, is the probability density function which in this case is the univariate normal distribution given by,\n\n$$\nf(x,\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^\\frac{-(x-\\mu)^2}{2\\sigma^2}\n$$\n\nThe likelihood of a distribution with given mean and variance is the product of value of PDF at all sample points with given mean and variance. Likelihood gives us a fair estimate or probablity as to how well the distribution is fitting the data for given parameters (mean and variance in case of normal distributon). So the probability that the normal distribution with given mean and variance will fit the samples is a product of the value of the pdf at each sample point as all these events are independant events. Hence, plotting the likelihood or the log-likelihood as function of parameters (in this case mean and variance) can give us an estimate as to which values of parameters are needed to best fit the data.\n\nLikelihoods are generally products of many numbers and products are not numerically stable. Hence, log-likelihood is calculated to make computations less expensive and simpler.\n\nTaking log on both sides,\n\n$$\n\\begin{equation} \\begin{split}\nl(\\mu,\\sigma^2;x_1,x_2,...,x_n) & = ln(L(\\mu,\\sigma^2;x_1,x_2,...,x_n)) \\\\\n & = ln((2\\pi\\sigma^2)^\\frac{-n}{2} e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2})\\\\ & = ln(2\\pi\\sigma^2)^\\frac{-n}{2}+ln(e^\\frac{-\\Sigma_{j=1}^n (x_j-\\mu)^2}{2\\sigma^2}))\\\\ &= \\frac{-n}{2}ln(2\\pi\\sigma^2)-\\Sigma_{j=1}^n \\frac{(x_j-\\mu)^2}{2\\sigma^2}  \\end{split} \\end{equation}\n$$\n\nThe above equation can be used to compute the log-likelihood.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nlog_likelihoods = []\nfor mean in np.linspace(0, 2, 100):\n    for var in np.linspace(0.1, 2, 100):\n        log_likelihood = np.sum(-0.5 * np.log(2 * np.pi * var) - ((samples - mean) ** 2) / (2 * var))\n        log_likelihoods.append((mean, var, log_likelihood))\n\nlog_likelihoods = np.array(log_likelihoods)\nprint(log_likelihoods.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10000, 3)\n```\n:::\n:::\n\n\nThe log_likelihoods array is a 2D-array with mean values in the first column, variance values in the second column and likelihood values in the third column. Let's look at the first 5 rows of the array.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nlog_likelihoods[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[ 0.00000000e+00,  1.00000000e-01, -9.97514834e+03],\n       [ 0.00000000e+00,  1.19191919e-01, -8.41934984e+03],\n       [ 0.00000000e+00,  1.38383838e-01, -7.30630086e+03],\n       [ 0.00000000e+00,  1.57575758e-01, -6.47285210e+03],\n       [ 0.00000000e+00,  1.76767677e-01, -5.82700895e+03]])\n```\n:::\n:::\n\n\nA 3D plot of how log-likelihood varies with mean and variance can be seen below.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.plot3D(log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2],color='green')\nax.set_title(\"Log-likelihood of normal distribution as a function of mean and variance\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nax.set_zlabel(\"Log-likelihood\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index 2_files/figure-html/cell-7-output-1.png){width=595 height=416}\n:::\n:::\n\n\nWe can also visualize 3D plots in 2D using a contour plot. To do so, we can create a 100x100 grid (because we have 100 mean and variance values) and reshape Z's shape to 100x100 so we have a value of log-likelihood for each point on the grid.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Contour plot\nfig,ax = plt.subplots()\nX,Y,Z = log_likelihoods[:,0],log_likelihoods[:,1],log_likelihoods[:,2]\nX,Y = np.meshgrid(np.unique(X),np.unique(Y))\nZ = Z.reshape(X.shape)\nplt.contourf(X,Y,Z)\nplt.colorbar()\nax.set_title(\"Contour plot\")\nax.set_xlabel(\"Mean\")\nax.set_ylabel(\"Variance\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index 2_files/figure-html/cell-8-output-1.png){width=600 height=449}\n:::\n:::\n\n\nThe values of mean and variance which will best fit the data will be those having the highest value of log-likelihood (Z-axis) and hence in the \"yellow\" region of the contour plot.\n\n",
    "supporting": [
      "index 2_files"
    ],
    "filters": [],
    "includes": {}
  }
}